{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"15VAE_translation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"a7c29c5b-77c0-4965-a971-f2435f353c58","executionInfo":{"status":"ok","timestamp":1558572759006,"user_tz":-480,"elapsed":44090,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":541}},"source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==2.0.0-alpha0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n","\u001b[K     |████████████████████████████████| 332.1MB 79kB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n","Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 45.7MB/s \n","\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n","\u001b[K     |████████████████████████████████| 419kB 57.2MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n","Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 33.1MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","Installing collected packages: tb-nightly, tf-estimator-nightly, google-pasta, tensorflow-gpu\n","Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n","/device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"853408bb-8052-43f9-925c-a92981e710ba","executionInfo":{"status":"ok","timestamp":1558572973675,"user_tz":-480,"elapsed":258735,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"source":["batch_size = 32\n","emb_size = 300\n","hidden_size = 512\n","lr = 1e-3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"source":["train_en_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/train.en.txt' # train set\n","train_ja_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/train.ja.txt' # train set\n","dev_en_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/dev.en.txt' # dev set\n","dev_ja_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/dev.ja.txt' # dev set\n","test_en_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/test.en.txt' \n","test_ja_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/para/test.ja.txt' "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"source":["def read_data(file):\n","    with open(file, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words = line.lower().strip()\n","            words = '<start> ' + words + ' <end>'\n","            data.append(words)\n","        return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"alGOxQuM5F4R","colab_type":"code","outputId":"a705e80e-ca4a-45af-9b74-f0c4ac1f485a","executionInfo":{"status":"ok","timestamp":1558572975760,"user_tz":-480,"elapsed":260801,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_en = read_data(train_en_path)\n","train_ja = read_data(train_ja_path)\n","dev_en = read_data(dev_en_path)\n","dev_ja = read_data(dev_ja_path)\n","test_en = read_data(test_en_path)\n","test_ja = read_data(test_ja_path)\n","print (len(train_en), len(dev_en), len(test_en))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["10000 500 500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dbs7HmMcDy1n","colab_type":"code","colab":{}},"source":["tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>') \n","tokenizer_en.fit_on_texts(train_en)\n","train_seq_en = tokenizer_en.texts_to_sequences(train_en)\n","dev_seq_en = tokenizer_en.texts_to_sequences(dev_en)\n","test_seq_en = tokenizer_en.texts_to_sequences(test_en)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","outputId":"59e701c1-9e08-412d-a4f2-02ce302c837d","executionInfo":{"status":"ok","timestamp":1558572975770,"user_tz":-480,"elapsed":260798,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w2i_en = tokenizer_en.word_index\n","i2w_en = dict([(value, key) for (key, value) in w2i_en.items()])\n","nwords_en = len(w2i_en)\n","print (nwords_en)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["7043\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oyizsfjoMWv6","colab_type":"code","colab":{}},"source":["tokenizer_ja = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>') \n","tokenizer_ja.fit_on_texts(train_ja)\n","train_seq_ja = tokenizer_ja.texts_to_sequences(train_ja)\n","dev_seq_ja = tokenizer_ja.texts_to_sequences(dev_ja)\n","test_seq_ja = tokenizer_ja.texts_to_sequences(test_ja)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Dftg4t3QCZW","colab_type":"code","outputId":"4be12b92-62f8-4df4-e2a4-2c45eb61e4af","executionInfo":{"status":"ok","timestamp":1558572976063,"user_tz":-480,"elapsed":261082,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w2i_ja = tokenizer_ja.word_index\n","i2w_ja = dict([(value, key) for (key, value) in w2i_ja.items()])\n","nwords_ja = len(w2i_ja)\n","print (nwords_ja)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["8061\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qP_CCbf8upgZ","colab_type":"code","outputId":"543e73e3-ba84-4862-ef1d-b77c8952c97d","executionInfo":{"status":"ok","timestamp":1558572976066,"user_tz":-480,"elapsed":261075,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w2i_en['<start>'], i2w_ja[2]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, '。')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"m5G-VOxPlmhQ","colab_type":"code","colab":{}},"source":["def batch_gen(src, tar):\n","    i = 0\n","    while i<len(src):\n","        if i+batch_size < len(src):\n","            batch_src = tf.keras.preprocessing.sequence.pad_sequences(src[i:i+batch_size], padding='post')\n","            batch_tar = tf.keras.preprocessing.sequence.pad_sequences(tar[i:i+batch_size], padding='post')\n","        else:\n","            batch_src = tf.keras.preprocessing.sequence.pad_sequences(src[i:], padding='post')\n","            batch_tar = tf.keras.preprocessing.sequence.pad_sequences(tar[i:], padding='post')\n","        i += batch_size\n","        yield batch_src, batch_tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLgNCpBmw2fy","colab_type":"code","colab":{}},"source":["def shuffle_seq(src, tar):\n","    c = list(zip(src, tar))\n","    random.shuffle(c)\n","    src, tar = zip(*c)\n","    return src, tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"83WRgQ77VEVL","colab_type":"code","colab":{}},"source":["# encode the source sentence to latent variable, get the KL divergence\n","class Encoder(tf.keras.Model):\n","    def __init__(self, nwords, embed_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(nwords+1, embed_size, trainable=True, mask_zero=True)\n","        self.lstm = tf.keras.layers.LSTM(hidden_size, recurrent_initializer='glorot_uniform')\n","        self.dense1_mean = tf.keras.layers.Dense(hidden_size, activation='tanh')\n","        self.dense2_mean = tf.keras.layers.Dense(hidden_size, use_bias=False)\n","        self.dense1_var = tf.keras.layers.Dense(hidden_size, activation='tanh')\n","        self.dense2_var = tf.keras.layers.Dense(hidden_size, use_bias=False)\n","        \n","    def kl_loss(self, mu, log_var): #[batch, hid]\n","        kl = 1 + log_var - mu * mu - tf.exp(log_var) #[batch, hid]\n","        kl = tf.reduce_sum(kl, axis=-1) #[batch]\n","        return -0.5 * kl\n","    \n","    # reparameterization trick: get the latent variable from the mean vector and variance vector\n","    def reparameterize(self, mu, log_var): #[batch, hid]\n","        std = tf.exp(log_var * 0.5) #[batch, hid]\n","        eps = tf.random.normal(tf.shape(mu)) #[batch, hid]\n","        z = mu + std * eps #[batch, hid]\n","        return z\n","\n","    def call(self, x, mask): #[batch, t]\n","        x = self.embedding(x) #[batch, t, emb]\n","        # passing all time steps to the lstm, get the last output\n","        output = self.lstm(x, mask=mask) # [batch, hidden]\n","        # mean of the latent variable\n","        mu = self.dense1_mean(output)\n","        mu = self.dense2_mean(mu) #[batch, hid]\n","        # log variance of the latent variable\n","        log_var = self.dense1_var(output)\n","        log_var = self.dense2_var(log_var) #[batch, hid]\n","        \n","        kl = self.kl_loss(mu, log_var) #[batch]\n","        z = self.reparameterize(mu, log_var) #[batch, hid]\n","\n","        return kl, z"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lFQaSdzZFzS","colab_type":"code","outputId":"66b6e5f4-2900-4d7b-d032-a43ccbbb6fc3","executionInfo":{"status":"ok","timestamp":1558572976349,"user_tz":-480,"elapsed":261337,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["encoder = Encoder(nwords_en, emb_size, hidden_size)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0523 00:56:16.095211 139944518780800 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f46ed5177f0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CG-wy0-nLTCS","colab_type":"code","colab":{}},"source":["# decode to get the target sentences. use latent variable as initial states\n","class Decoder(tf.keras.Model):\n","    def __init__(self, nwords, embed_size, hidden_size):\n","        super(Decoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(nwords+1, embed_size, trainable=True, mask_zero=True)\n","        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, recurrent_initializer='glorot_uniform')\n","        self.fc = tf.keras.layers.Dense(nwords+1)\n","\n","    def call(self, x, mask, init_hidden, init_cell): # x:[batch, t], init_hidden:[batch, hid], init_cell:[batch, hid]\n","        x = self.embedding(x) #[batch, t, emb]\n","        # passing all time steps to the LSTM\n","        outputs = self.lstm(x, mask=mask, initial_state=[init_hidden, init_cell]) #[batch, t, hid]\n","        outputs = self.fc(outputs) #[batch, t, nwords+1]\n","\n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mazHuxpwZQim","colab_type":"code","outputId":"8ce204ed-4316-4424-8645-fadc6c63d1cd","executionInfo":{"status":"ok","timestamp":1558572976352,"user_tz":-480,"elapsed":261321,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["decoder = Decoder(nwords_ja, emb_size, hidden_size)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["W0523 00:56:16.134467 139944518780800 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f46e0241518>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"d67F0-euFTbP","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# get the reconstruction loss\n","def loss_function(real, pred, mask): # real:[batch, t], pred:[batch, t, nwords+1], mask: [batch, t]\n","    reconstruction_loss = loss_object(real, pred) #[batch, t]\n","\n","    mask = tf.cast(mask, dtype=reconstruction_loss.dtype)\n","    reconstruction_loss *= mask #[batch, t]\n","\n","    return tf.reduce_sum(reconstruction_loss, axis=-1) #[batch]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D73f_jv7cis_","colab_type":"code","colab":{}},"source":["def train_step(src, tar): # scr, tar: [batch, t]\n","    loss = 0  \n","    with tf.GradientTape() as tape:\n","        mask_src = tf.math.logical_not(tf.math.equal(src, 0)) #[batch, t]\n","        kl, z = encoder(src, mask_src) # kl: [batch], z: [batch, hid]\n","        init_hidden = z\n","        init_cell = tf.tanh(z)\n","        mask_tar = tf.math.logical_not(tf.math.equal(tar, 0)) #[batch, t]\n","        logits = decoder(tar, mask_tar, init_hidden, init_cell) #[batch, t, nwords+1]\n","        mask_loss = tf.math.logical_not(tf.math.logical_or(tf.math.equal(tar, 0), tf.math.equal(tar, w2i_ja['<end>']))) #[batch, t]\n","        tar_real = tf.concat([tar[:, 1:], tf.zeros([tf.shape(tar)[0], 1])], axis=-1) # [batch, t] shift the target sequence one word to left\n","        recon = loss_function(tar_real, logits, mask_loss) #[batch]\n","        loss = tf.reduce_mean(recon + kl)\n","\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","  \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRDxn80qlBYN","colab_type":"code","colab":{}},"source":["def eval_step(src, tar):\n","    loss = 0    \n","    mask_src = tf.math.logical_not(tf.math.equal(src, 0)) #[batch, t]\n","    kl, z = encoder(src, mask_src) # kl: [batch], z: [batch, hid]\n","    init_hidden = z\n","    init_cell = tf.tanh(z)\n","    mask_tar = tf.math.logical_not(tf.math.equal(tar, 0)) #[batch, t]\n","    logits = decoder(tar, mask_tar, init_hidden, init_cell) #[batch, t, nwords+1]\n","    mask_loss = tf.math.logical_not(tf.math.logical_or(tf.math.equal(tar, 0), tf.math.equal(tar, w2i_ja['<end>']))) #[batch, t]\n","    tar_real = tf.concat([tar[:, 1:], tf.zeros([tf.shape(tar)[0], 1])], axis=-1) # [batch, t] shift the target sequence one word to left\n","    recon = loss_function(tar_real, logits, mask_loss) #[batch]\n","    loss = tf.reduce_mean(recon + kl)\n","  \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKhGD5-EL9yb","colab_type":"code","outputId":"21b1cd0f-f778-4c6c-efc6-dcd4ac4a753b","executionInfo":{"status":"ok","timestamp":1558574809857,"user_tz":-480,"elapsed":2094816,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":1361}},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    print ('Epoch {} start ...... '.format(epoch + 1))\n","    train_seq_en, train_seq_ja = shuffle_seq(train_seq_en, train_seq_ja)\n","    total_loss = 0\n","    batch = 0\n","    for src, tar in batch_gen(train_seq_en, train_seq_ja):\n","        batch_loss = train_step(src, tar)\n","        total_loss += batch_loss\n","        batch += 1\n","\n","        if batch % 50 == 0:\n","            print('  ---- Batch {} Loss {:.4f}'.format(batch, batch_loss))\n","    \n","    # Evaluate on dev set\n","    dev_loss = 0\n","    batch_eval = 0\n","    for src, tar in batch_gen(dev_seq_en, dev_seq_ja):\n","        batch_loss = eval_step(src, tar)\n","        dev_loss += batch_loss\n","        batch_eval += 1\n","\n","    print('Epoch {} finished in {} seconds. Training loss {:.4f}. Evaluation loss {:.4f}.'.format(epoch + 1, time.time() - start, total_loss / batch, dev_loss / batch_eval))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch 1 start ...... \n","  ---- Batch 50 Loss 58.5305\n","  ---- Batch 100 Loss 50.5471\n","  ---- Batch 150 Loss 37.6738\n","  ---- Batch 200 Loss 44.6816\n","  ---- Batch 250 Loss 36.2839\n","  ---- Batch 300 Loss 29.3525\n","Epoch 1 finished in 187.98851895332336 seconds. Training loss 44.1022. Evaluation loss 30.9588.\n","Epoch 2 start ...... \n","  ---- Batch 50 Loss 27.5488\n","  ---- Batch 100 Loss 24.3035\n","  ---- Batch 150 Loss 32.2130\n","  ---- Batch 200 Loss 38.3843\n","  ---- Batch 250 Loss 33.1509\n","  ---- Batch 300 Loss 25.8478\n","Epoch 2 finished in 183.21207976341248 seconds. Training loss 29.7858. Evaluation loss 27.2449.\n","Epoch 3 start ...... \n","  ---- Batch 50 Loss 27.7245\n","  ---- Batch 100 Loss 27.7768\n","  ---- Batch 150 Loss 21.3044\n","  ---- Batch 200 Loss 28.0052\n","  ---- Batch 250 Loss 29.6176\n","  ---- Batch 300 Loss 27.8073\n","Epoch 3 finished in 184.0655255317688 seconds. Training loss 26.6978. Evaluation loss 25.9557.\n","Epoch 4 start ...... \n","  ---- Batch 50 Loss 31.3521\n","  ---- Batch 100 Loss 27.5336\n","  ---- Batch 150 Loss 22.4223\n","  ---- Batch 200 Loss 18.9274\n","  ---- Batch 250 Loss 20.6194\n","  ---- Batch 300 Loss 28.7691\n","Epoch 4 finished in 182.5290024280548 seconds. Training loss 24.6800. Evaluation loss 25.3464.\n","Epoch 5 start ...... \n","  ---- Batch 50 Loss 15.0334\n","  ---- Batch 100 Loss 25.4833\n","  ---- Batch 150 Loss 18.6440\n","  ---- Batch 200 Loss 26.4435\n","  ---- Batch 250 Loss 27.0174\n","  ---- Batch 300 Loss 27.9693\n","Epoch 5 finished in 183.69238996505737 seconds. Training loss 23.6418. Evaluation loss 25.0187.\n","Epoch 6 start ...... \n","  ---- Batch 50 Loss 25.5560\n","  ---- Batch 100 Loss 18.5459\n","  ---- Batch 150 Loss 23.4206\n","  ---- Batch 200 Loss 22.4022\n","  ---- Batch 250 Loss 17.3198\n","  ---- Batch 300 Loss 18.1652\n","Epoch 6 finished in 183.19094705581665 seconds. Training loss 22.4835. Evaluation loss 24.8732.\n","Epoch 7 start ...... \n","  ---- Batch 50 Loss 19.4095\n","  ---- Batch 100 Loss 25.4781\n","  ---- Batch 150 Loss 21.4837\n","  ---- Batch 200 Loss 15.7811\n","  ---- Batch 250 Loss 24.3892\n","  ---- Batch 300 Loss 18.3258\n","Epoch 7 finished in 180.87750005722046 seconds. Training loss 21.5556. Evaluation loss 24.8450.\n","Epoch 8 start ...... \n","  ---- Batch 50 Loss 19.9236\n","  ---- Batch 100 Loss 17.0713\n","  ---- Batch 150 Loss 24.4984\n","  ---- Batch 200 Loss 18.1105\n","  ---- Batch 250 Loss 18.0786\n","  ---- Batch 300 Loss 16.1959\n","Epoch 8 finished in 181.41782784461975 seconds. Training loss 20.7479. Evaluation loss 24.8211.\n","Epoch 9 start ...... \n","  ---- Batch 50 Loss 20.0731\n","  ---- Batch 100 Loss 21.0923\n","  ---- Batch 150 Loss 19.3943\n","  ---- Batch 200 Loss 16.2127\n","  ---- Batch 250 Loss 24.4639\n","  ---- Batch 300 Loss 17.7654\n","Epoch 9 finished in 182.13768863677979 seconds. Training loss 19.5469. Evaluation loss 24.9952.\n","Epoch 10 start ...... \n","  ---- Batch 50 Loss 17.2345\n","  ---- Batch 100 Loss 18.5461\n","  ---- Batch 150 Loss 16.1100\n","  ---- Batch 200 Loss 21.6784\n","  ---- Batch 250 Loss 21.3094\n","  ---- Batch 300 Loss 19.0551\n","Epoch 10 finished in 184.0471374988556 seconds. Training loss 18.7155. Evaluation loss 25.1465.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gMVXMVD9ltkp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}