{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02lm.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"c5b38684-c7c3-4551-9114-8242d22f2bd0","executionInfo":{"status":"ok","timestamp":1553589686883,"user_tz":-480,"elapsed":8651,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":397}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.4)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n","Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.14.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n","/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"f404a610-6f51-4d26-d3ad-a1c15d8a8009","executionInfo":{"status":"ok","timestamp":1553589708597,"user_tz":-480,"elapsed":30325,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 64\n","emb_size = 50\n","ngram = 2\n","dropout = 0.5\n","lr = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/train.txt' # train set\n","dev_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/valid.txt' # dev set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_dataset(filename):\n","    with open(filename, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words = line.lower().strip()\n","            data.append(words)\n","        return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"alGOxQuM5F4R","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = read_dataset(train_path)\n","random.shuffle(train_set)\n","dev_set = read_dataset(dev_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ryQ1UiUBx8LP","colab_type":"code","colab":{}},"cell_type":"code","source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>')\n","tokenizer.fit_on_texts(train_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","outputId":"0b53467d-f0e8-4c99-f3d3-80840c56e104","executionInfo":{"status":"ok","timestamp":1553589710182,"user_tz":-480,"elapsed":31840,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["nwords = len(tokenizer.word_index)\n","print (nwords)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["9650\n"],"name":"stdout"}]},{"metadata":{"id":"m5G-VOxPlmhQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_gen(batch_size=batch_size):\n","    steps = len(train_set) // batch_size\n","    for step in range(steps):\n","        train_seq = tokenizer.texts_to_sequences(train_set[step:step+batch_size])\n","        yield train_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DY4TgO-BDEQ1","colab_type":"code","colab":{}},"cell_type":"code","source":["def dev_gen(batch_size=batch_size):\n","    steps = len(dev_set) // batch_size\n","    for step in range(steps):\n","        dev_seq = tokenizer.texts_to_sequences(dev_set[step:step+batch_size])\n","        yield dev_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CG-wy0-nLTCS","colab_type":"code","colab":{}},"cell_type":"code","source":["class NNLM(tf.keras.Model):\n","    def __init__(self, nwords, emb_size, ngram, dropout):\n","        super(NNLM, self).__init__()\n","        self.w = self.add_weight(shape=(nwords+1, emb_size),\n","                                 initializer='glorot_normal',\n","                                 trainable=True)\n","        self.b = self.add_weight(shape=(nwords+1,),\n","                                 initializer='zeros',\n","                                 trainable=True)\n","        self.dense = tf.keras.layers.Dense(emb_size, activation='tanh')\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","        self.softmax = tf.keras.layers.Softmax()\n","\n","    @tf.function\n","    def call(self, x):\n","        \"\"\"Run the model.\"\"\"\n","        result = tf.nn.embedding_lookup(self.w, x)\n","        result = tf.reshape(result, [x.shape[0], -1])\n","        result = self.dense(result)\n","        result = self.dropout(result)\n","        result = tf.matmul(result, tf.transpose(self.w)) + self.b\n","        result = self.softmax(result)\n","        return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mazHuxpwZQim","colab_type":"code","colab":{}},"cell_type":"code","source":["model = NNLM(nwords, emb_size, ngram, dropout)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D73f_jv7cis_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Calculate the loss value for the whole batch of sentences\n","def sent_loss(sents):\n","    all_histories = []\n","    all_targets = []\n","    for sent in sents:\n","        hist = [0] * ngram\n","        for next_word in sent + [0]:\n","            all_histories.append(list(hist))\n","            all_targets.append(next_word)\n","            hist = hist[1:] + [next_word]\n","\n","    all_histories = tf.Variable(all_histories, trainable=False)\n","    all_targets = tf.Variable(all_targets, trainable=False)\n","    logits = model(all_histories)\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.losses.Reduction.SUM)\n","    loss = loss_fn(all_targets, logits)\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2mH34W35lGVu","colab_type":"code","outputId":"5fdff820-5310-4a35-90bc-f74e689827bf","executionInfo":{"status":"error","timestamp":1553591400344,"user_tz":-480,"elapsed":1721958,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":4943}},"cell_type":"code","source":["last_dev = 1e20\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# Iterate over epochs.\n","for epoch in range(15):\n","    print('Start of epoch %d' % (epoch,))\n","    start = time.time()\n","    train_loss = 0\n","    train_words = 0\n","    batch_id = 0\n","    for sents in train_gen():\n","\n","        # Open a GradientTape to record the operations run during the forward pass, which enables autodifferentiation.\n","        with tf.GradientTape() as tape:\n","\n","            loss = sent_loss(sents)\n","            train_loss += loss\n","            batch_words = sum(list(map(len, sents)))\n","            train_words += batch_words\n","\n","            # Use the gradient tape to automatically retrieve the gradients of the trainable weights with respect to the loss.\n","            grads = tape.gradient(loss, model.trainable_variables)\n","\n","            # Run one step of gradient descent by updating the value of the weights to minimize the loss.\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","            # Log every 20 batch.\n","            batch_id += 1\n","            if batch_id % 20 == 0:\n","                print('Training time: %0.3f seconds, training loss at sentence %d: %0.4f' % (time.time()-start, batch_id*batch_size, loss/batch_words))\n","                \n","    print ('Epoch %d: Training time=%0.3f seconds, training loss per word=%0.4f' % (epoch, time.time()-start, train_loss/train_words))\n","    \n","    # Evaluate on dev set\n","    dev_words, dev_loss = 0, 0\n","    for sents in dev_gen():\n","        loss = sent_loss(sents)\n","        dev_loss += loss\n","        dev_words += sum(list(map(len, sents)))\n","    print ('Epoch %d: evaluation loss per word=%f' % (epoch, dev_loss/dev_words))\n","    \n","    # Keep track of the development accuracy and reduce the learning rate if it got worse\n","    if last_dev < dev_loss/dev_words:\n","        lr /= 2\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","    last_dev = dev_loss/dev_words"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start of epoch 0\n","Training time: 8.284 seconds, training loss at sentence 1280: 9.5427\n","Training time: 16.154 seconds, training loss at sentence 2560: 9.0242\n","Training time: 23.977 seconds, training loss at sentence 3840: 7.9139\n","Training time: 31.789 seconds, training loss at sentence 5120: 7.1955\n","Training time: 39.708 seconds, training loss at sentence 6400: 7.1883\n","Training time: 47.321 seconds, training loss at sentence 7680: 7.1057\n","Training time: 55.236 seconds, training loss at sentence 8960: 6.9779\n","Training time: 62.851 seconds, training loss at sentence 10240: 6.7675\n","Training time: 70.811 seconds, training loss at sentence 11520: 6.6592\n","Training time: 78.454 seconds, training loss at sentence 12800: 6.5506\n","Training time: 86.418 seconds, training loss at sentence 14080: 6.4672\n","Training time: 93.946 seconds, training loss at sentence 15360: 6.5011\n","Training time: 101.920 seconds, training loss at sentence 16640: 6.4323\n","Training time: 109.534 seconds, training loss at sentence 17920: 6.3757\n","Training time: 117.226 seconds, training loss at sentence 19200: 6.3116\n","Training time: 125.545 seconds, training loss at sentence 20480: 6.2696\n","Training time: 133.304 seconds, training loss at sentence 21760: 6.1697\n","Training time: 141.010 seconds, training loss at sentence 23040: 6.3129\n","Training time: 148.724 seconds, training loss at sentence 24320: 6.4104\n","Training time: 157.144 seconds, training loss at sentence 25600: 6.3210\n","Training time: 164.902 seconds, training loss at sentence 26880: 6.2241\n","Training time: 172.620 seconds, training loss at sentence 28160: 6.1589\n","Training time: 180.395 seconds, training loss at sentence 29440: 6.0825\n","Training time: 188.166 seconds, training loss at sentence 30720: 6.1120\n","Training time: 196.706 seconds, training loss at sentence 32000: 6.1483\n","Training time: 204.421 seconds, training loss at sentence 33280: 6.2398\n","Training time: 212.274 seconds, training loss at sentence 34560: 6.1339\n","Training time: 220.233 seconds, training loss at sentence 35840: 6.0840\n","Training time: 228.159 seconds, training loss at sentence 37120: 5.9810\n","Training time: 237.037 seconds, training loss at sentence 38400: 6.0582\n","Training time: 244.865 seconds, training loss at sentence 39680: 6.0283\n","Training time: 252.663 seconds, training loss at sentence 40960: 6.2174\n","Epoch 0: Training time=259.339 seconds, training loss per word=6.6550\n","Epoch 0: evaluation loss per word=7.795930\n","Start of epoch 1\n","Training time: 7.823 seconds, training loss at sentence 1280: 6.1647\n","Training time: 16.747 seconds, training loss at sentence 2560: 5.8711\n","Training time: 24.617 seconds, training loss at sentence 3840: 5.8570\n","Training time: 32.487 seconds, training loss at sentence 5120: 5.7109\n","Training time: 40.365 seconds, training loss at sentence 6400: 5.7975\n","Training time: 48.267 seconds, training loss at sentence 7680: 5.7430\n","Training time: 56.141 seconds, training loss at sentence 8960: 5.7076\n","Training time: 64.061 seconds, training loss at sentence 10240: 5.6754\n","Training time: 71.999 seconds, training loss at sentence 11520: 5.6095\n","Training time: 81.331 seconds, training loss at sentence 12800: 5.4819\n","Training time: 89.259 seconds, training loss at sentence 14080: 5.3798\n","Training time: 97.184 seconds, training loss at sentence 15360: 5.4683\n","Training time: 105.021 seconds, training loss at sentence 16640: 5.3907\n","Training time: 112.865 seconds, training loss at sentence 17920: 5.3427\n","Training time: 120.804 seconds, training loss at sentence 19200: 5.2924\n","Training time: 128.787 seconds, training loss at sentence 20480: 5.1505\n","Training time: 136.694 seconds, training loss at sentence 21760: 5.0638\n","Training time: 144.602 seconds, training loss at sentence 23040: 5.2050\n","Training time: 154.155 seconds, training loss at sentence 24320: 5.3191\n","Training time: 162.054 seconds, training loss at sentence 25600: 5.1382\n","Training time: 169.946 seconds, training loss at sentence 26880: 4.9763\n","Training time: 177.925 seconds, training loss at sentence 28160: 4.9028\n","Training time: 185.779 seconds, training loss at sentence 29440: 4.8522\n","Training time: 193.630 seconds, training loss at sentence 30720: 4.9334\n","Training time: 201.510 seconds, training loss at sentence 32000: 4.9765\n","Training time: 209.374 seconds, training loss at sentence 33280: 5.0116\n","Training time: 217.336 seconds, training loss at sentence 34560: 4.8813\n","Training time: 225.339 seconds, training loss at sentence 35840: 4.8238\n","Training time: 233.351 seconds, training loss at sentence 37120: 4.6842\n","Training time: 241.299 seconds, training loss at sentence 38400: 4.7219\n","Training time: 251.371 seconds, training loss at sentence 39680: 4.7477\n","Training time: 259.383 seconds, training loss at sentence 40960: 5.0118\n","Epoch 1: Training time=266.213 seconds, training loss per word=5.2989\n","Epoch 1: evaluation loss per word=7.891155\n","Start of epoch 2\n","Training time: 7.873 seconds, training loss at sentence 1280: 5.7963\n","Training time: 15.732 seconds, training loss at sentence 2560: 5.5992\n","Training time: 23.782 seconds, training loss at sentence 3840: 5.5753\n","Training time: 31.741 seconds, training loss at sentence 5120: 5.4181\n","Training time: 39.747 seconds, training loss at sentence 6400: 5.3531\n","Training time: 47.716 seconds, training loss at sentence 7680: 5.2897\n","Training time: 55.660 seconds, training loss at sentence 8960: 5.2151\n","Training time: 63.619 seconds, training loss at sentence 10240: 5.1682\n","Training time: 71.601 seconds, training loss at sentence 11520: 5.0594\n","Training time: 79.614 seconds, training loss at sentence 12800: 4.9197\n","Training time: 90.113 seconds, training loss at sentence 14080: 4.8228\n","Training time: 98.160 seconds, training loss at sentence 15360: 4.8823\n","Training time: 106.176 seconds, training loss at sentence 16640: 4.8811\n","Training time: 114.174 seconds, training loss at sentence 17920: 4.8389\n","Training time: 122.248 seconds, training loss at sentence 19200: 4.8163\n","Training time: 130.307 seconds, training loss at sentence 20480: 4.7097\n","Training time: 138.315 seconds, training loss at sentence 21760: 4.6410\n","Training time: 146.316 seconds, training loss at sentence 23040: 4.7383\n","Training time: 154.314 seconds, training loss at sentence 24320: 4.7907\n","Training time: 162.259 seconds, training loss at sentence 25600: 4.6818\n","Training time: 170.234 seconds, training loss at sentence 26880: 4.5339\n","Training time: 178.240 seconds, training loss at sentence 28160: 4.4529\n","Training time: 186.220 seconds, training loss at sentence 29440: 4.4091\n","Training time: 194.185 seconds, training loss at sentence 30720: 4.4157\n","Training time: 202.186 seconds, training loss at sentence 32000: 4.4503\n","Training time: 213.213 seconds, training loss at sentence 33280: 4.4448\n","Training time: 221.284 seconds, training loss at sentence 34560: 4.3040\n","Training time: 229.379 seconds, training loss at sentence 35840: 4.1968\n","Training time: 237.510 seconds, training loss at sentence 37120: 4.0815\n","Training time: 245.633 seconds, training loss at sentence 38400: 4.0814\n","Training time: 253.753 seconds, training loss at sentence 39680: 4.2249\n","Training time: 261.872 seconds, training loss at sentence 40960: 4.7332\n","Epoch 2: Training time=268.806 seconds, training loss per word=4.8218\n","Epoch 2: evaluation loss per word=8.001323\n","Start of epoch 3\n","Training time: 8.043 seconds, training loss at sentence 1280: 5.5271\n","Training time: 16.073 seconds, training loss at sentence 2560: 5.3088\n","Training time: 24.193 seconds, training loss at sentence 3840: 5.2863\n","Training time: 32.256 seconds, training loss at sentence 5120: 5.1705\n","Training time: 40.299 seconds, training loss at sentence 6400: 5.0901\n","Training time: 48.387 seconds, training loss at sentence 7680: 5.0226\n","Training time: 56.451 seconds, training loss at sentence 8960: 4.9418\n","Training time: 64.525 seconds, training loss at sentence 10240: 4.8921\n","Training time: 72.665 seconds, training loss at sentence 11520: 4.7726\n","Training time: 80.846 seconds, training loss at sentence 12800: 4.6272\n","Training time: 88.994 seconds, training loss at sentence 14080: 4.5244\n","Training time: 100.817 seconds, training loss at sentence 15360: 4.5525\n","Training time: 108.966 seconds, training loss at sentence 16640: 4.5586\n","Training time: 117.117 seconds, training loss at sentence 17920: 4.5234\n","Training time: 125.295 seconds, training loss at sentence 19200: 4.5122\n","Training time: 133.477 seconds, training loss at sentence 20480: 4.4316\n","Training time: 141.661 seconds, training loss at sentence 21760: 4.3995\n","Training time: 149.805 seconds, training loss at sentence 23040: 4.4595\n","Training time: 157.874 seconds, training loss at sentence 24320: 4.5030\n","Training time: 165.967 seconds, training loss at sentence 25600: 4.4493\n","Training time: 174.090 seconds, training loss at sentence 26880: 4.3114\n","Training time: 182.218 seconds, training loss at sentence 28160: 4.2407\n","Training time: 190.318 seconds, training loss at sentence 29440: 4.1997\n","Training time: 198.481 seconds, training loss at sentence 30720: 4.1886\n","Training time: 206.590 seconds, training loss at sentence 32000: 4.2080\n","Training time: 214.737 seconds, training loss at sentence 33280: 4.1638\n","Training time: 222.952 seconds, training loss at sentence 34560: 4.0635\n","Training time: 231.195 seconds, training loss at sentence 35840: 3.9658\n","Training time: 239.491 seconds, training loss at sentence 37120: 3.8555\n","Training time: 247.709 seconds, training loss at sentence 38400: 3.8625\n","Training time: 255.954 seconds, training loss at sentence 39680: 4.0762\n","Training time: 264.151 seconds, training loss at sentence 40960: 4.7067\n","Epoch 3: Training time=271.167 seconds, training loss per word=4.5724\n","Epoch 3: evaluation loss per word=7.964053\n","Start of epoch 4\n","Training time: 8.177 seconds, training loss at sentence 1280: 5.2724\n","Training time: 16.346 seconds, training loss at sentence 2560: 4.9645\n","Training time: 24.624 seconds, training loss at sentence 3840: 4.8972\n","Training time: 32.872 seconds, training loss at sentence 5120: 4.7771\n","Training time: 41.153 seconds, training loss at sentence 6400: 4.7037\n","Training time: 49.404 seconds, training loss at sentence 7680: 4.6404\n","Training time: 57.537 seconds, training loss at sentence 8960: 4.5520\n","Training time: 65.802 seconds, training loss at sentence 10240: 4.5167\n","Training time: 74.040 seconds, training loss at sentence 11520: 4.3995\n","Training time: 82.322 seconds, training loss at sentence 12800: 4.2528\n","Training time: 90.592 seconds, training loss at sentence 14080: 4.1531\n","Training time: 98.867 seconds, training loss at sentence 15360: 4.1713\n","Training time: 107.109 seconds, training loss at sentence 16640: 4.1721\n","Training time: 115.391 seconds, training loss at sentence 17920: 4.1344\n","Training time: 123.707 seconds, training loss at sentence 19200: 4.1328\n","Training time: 132.035 seconds, training loss at sentence 20480: 4.0453\n","Training time: 140.339 seconds, training loss at sentence 21760: 4.0148\n","Training time: 148.634 seconds, training loss at sentence 23040: 4.0402\n","Training time: 156.886 seconds, training loss at sentence 24320: 4.0643\n","Training time: 165.096 seconds, training loss at sentence 25600: 4.0077\n","Training time: 173.322 seconds, training loss at sentence 26880: 3.8842\n","Training time: 181.613 seconds, training loss at sentence 28160: 3.8306\n","Training time: 189.895 seconds, training loss at sentence 29440: 3.8193\n","Training time: 198.156 seconds, training loss at sentence 30720: 3.8333\n","Training time: 206.404 seconds, training loss at sentence 32000: 3.8731\n","Training time: 214.661 seconds, training loss at sentence 33280: 3.8238\n","Training time: 223.002 seconds, training loss at sentence 34560: 3.7365\n","Training time: 236.811 seconds, training loss at sentence 35840: 3.6454\n","Training time: 245.212 seconds, training loss at sentence 37120: 3.5697\n","Training time: 253.598 seconds, training loss at sentence 38400: 3.5903\n","Training time: 261.944 seconds, training loss at sentence 39680: 3.8270\n","Training time: 270.342 seconds, training loss at sentence 40960: 4.4636\n","Epoch 4: Training time=277.469 seconds, training loss per word=4.2162\n","Epoch 4: evaluation loss per word=8.089950\n","Start of epoch 5\n","Training time: 8.357 seconds, training loss at sentence 1280: 5.0719\n","Training time: 16.729 seconds, training loss at sentence 2560: 4.7809\n","Training time: 25.048 seconds, training loss at sentence 3840: 4.7229\n","Training time: 33.409 seconds, training loss at sentence 5120: 4.6366\n","Training time: 41.819 seconds, training loss at sentence 6400: 4.5574\n","Training time: 50.197 seconds, training loss at sentence 7680: 4.5024\n","Training time: 58.509 seconds, training loss at sentence 8960: 4.4236\n","Training time: 66.814 seconds, training loss at sentence 10240: 4.3986\n","Training time: 75.107 seconds, training loss at sentence 11520: 4.2904\n","Training time: 83.479 seconds, training loss at sentence 12800: 4.1567\n","Training time: 91.873 seconds, training loss at sentence 14080: 4.0564\n","Training time: 100.313 seconds, training loss at sentence 15360: 4.0557\n","Training time: 108.686 seconds, training loss at sentence 16640: 4.0569\n","Training time: 117.080 seconds, training loss at sentence 17920: 4.0133\n","Training time: 125.512 seconds, training loss at sentence 19200: 4.0137\n","Training time: 133.944 seconds, training loss at sentence 20480: 3.9328\n","Training time: 142.372 seconds, training loss at sentence 21760: 3.9027\n","Training time: 150.777 seconds, training loss at sentence 23040: 3.9037\n","Training time: 159.151 seconds, training loss at sentence 24320: 3.9317\n","Training time: 167.493 seconds, training loss at sentence 25600: 3.8977\n","Training time: 175.880 seconds, training loss at sentence 26880: 3.7804\n","Training time: 184.233 seconds, training loss at sentence 28160: 3.7323\n","Training time: 192.617 seconds, training loss at sentence 29440: 3.7226\n","Training time: 200.958 seconds, training loss at sentence 30720: 3.7378\n","Training time: 209.347 seconds, training loss at sentence 32000: 3.7796\n","Training time: 217.775 seconds, training loss at sentence 33280: 3.7142\n","Training time: 226.259 seconds, training loss at sentence 34560: 3.6426\n","Training time: 234.759 seconds, training loss at sentence 35840: 3.5619\n","Training time: 249.797 seconds, training loss at sentence 37120: 3.4872\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-a5cb1b687e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Use the gradient tape to automatically retrieve the gradients of the trainable weights with respect to the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Run one step of gradient descent by updating the value of the weights to minimize the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MaximumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MaximumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m   \u001b[0;34m\"\"\"Returns grad*(x > y, x <= y) with type of grad.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreater_equal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MaximumMinimumGrad\u001b[0;34m(op, grad, selector_op)\u001b[0m\n\u001b[1;32m   1089\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0mxgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m   \u001b[0mygrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m   \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mygrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   3229\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3231\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3232\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3233\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   9058\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9059\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9060\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9061\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9062\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1491,9651] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Select]"]}]},{"metadata":{"id":"JIqgcp51EEoW","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}