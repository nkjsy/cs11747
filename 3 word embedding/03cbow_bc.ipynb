{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03cbow_bc.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"571ed5a1-34fe-418e-e5dc-b64dea8f2e91","executionInfo":{"status":"ok","timestamp":1554365413405,"user_tz":-480,"elapsed":12240,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":397}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n","Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.4)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n","/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"7a735fbe-c3c2-44c8-d794-75ac4783e466","executionInfo":{"status":"ok","timestamp":1554365435546,"user_tz":-480,"elapsed":34344,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 32\n","emb_size = 100\n","ngram = 2\n","lr = 0.002"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/train.txt' # train set\n","dev_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/valid.txt' # dev set\n","embed_path = '/content/drive/My Drive/Colab Notebooks/cs11747/3/embeddings.tsv' # word vector\n","words_path = '/content/drive/My Drive/Colab Notebooks/cs11747/3/labels.tsv' # words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_dataset(filename):\n","    with open(filename, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words = line.lower().strip()\n","            data.append(words)\n","        return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"alGOxQuM5F4R","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = read_dataset(train_path)\n","random.shuffle(train_set)\n","dev_set = read_dataset(dev_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1RkYTXvNRLsz","colab_type":"code","outputId":"70b58912-2419-4f06-eb42-2e12100986e1","executionInfo":{"status":"ok","timestamp":1554365435572,"user_tz":-480,"elapsed":34317,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["len(train_set)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["42068"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"ryQ1UiUBx8LP","colab_type":"code","colab":{}},"cell_type":"code","source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","outputId":"9c41b318-86bb-420c-fa2b-41e1635d1c37","executionInfo":{"status":"ok","timestamp":1554365436331,"user_tz":-480,"elapsed":35062,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_index = tokenizer.word_index\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","nwords = len(word_index)\n","print (nwords)\n","nbits = len(np.binary_repr(nwords))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["9649\n"],"name":"stdout"}]},{"metadata":{"id":"m5G-VOxPlmhQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_gen(batch_size=batch_size):\n","    steps = len(train_set) // batch_size\n","    for step in range(steps):\n","        train_seq = tokenizer.texts_to_sequences(train_set[step:step+batch_size])\n","        yield train_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DY4TgO-BDEQ1","colab_type":"code","colab":{}},"cell_type":"code","source":["def dev_gen(batch_size=batch_size):\n","    steps = len(dev_set) // batch_size\n","    for step in range(steps):\n","        dev_seq = tokenizer.texts_to_sequences(dev_set[step:step+batch_size])\n","        yield dev_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CG-wy0-nLTCS","colab_type":"code","colab":{}},"cell_type":"code","source":["class WordEmbCBOW(tf.keras.Model):\n","    def __init__(self, nwords, emb_size):\n","        super(WordEmbCBOW, self).__init__()\n","        self.embed = tf.keras.layers.Embedding(nwords+1, emb_size, trainable=True)\n","        self.dense = tf.keras.layers.Dense(nbits, activation='sigmoid')\n","\n","    @tf.function\n","    def call(self, x):\n","        \"\"\"Run the model.\"\"\"\n","        result = self.embed(x)\n","        result = tf.reduce_sum(result, axis=1) #[batch, emb]\n","        result = self.dense(result) #[batch, nbits]\n","        return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mazHuxpwZQim","colab_type":"code","colab":{}},"cell_type":"code","source":["model = WordEmbCBOW(nwords, emb_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D73f_jv7cis_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Calculate the loss value for the whole batch of sentences\n","def sent_loss(sents):\n","    all_windows = []\n","    all_targets = []\n","    for sent in sents:\n","        padded = [0] * ngram + sent + [0] * ngram\n","        for i in range(ngram, len(sent) + ngram):\n","            window = padded[i-ngram:i] + padded[i+1:i+ngram+1]\n","            all_windows.append(window)\n","            all_targets.append(padded[i])\n","\n","    all_windows = tf.Variable(all_windows, trainable=False)\n","    logits = model(all_windows)\n","    all_bc = [np.binary_repr(i).zfill(nbits) for i in all_targets]\n","    all_bc = [[float(s) for s in bc] for bc in all_bc]\n","    all_bc = tf.Variable(all_bc, trainable=False)\n","    loss_fn = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)\n","    loss = loss_fn(all_bc, logits)\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2mH34W35lGVu","colab_type":"code","outputId":"ed9c4db6-c121-4e6e-d462-affd23067b8a","executionInfo":{"status":"error","timestamp":1553664330194,"user_tz":-480,"elapsed":1123993,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":1673}},"cell_type":"code","source":["last_dev = 1e20\n","best_dev = 1e20\n","optimizer = tf.keras.optimizers.Adam(lr=lr)\n","# Iterate over epochs.\n","for epoch in range(15):\n","    print('Start of epoch %d' % (epoch,))\n","    start = time.time()\n","    train_loss = 0\n","    train_words = 0\n","    batch_id = 0\n","    for sents in train_gen():\n","\n","        # Open a GradientTape to record the operations run during the forward pass, which enables autodifferentiation.\n","        with tf.GradientTape() as tape:\n","\n","            loss = sent_loss(sents)\n","            train_loss += loss\n","            batch_words = sum(list(map(len, sents)))\n","            train_words += batch_words\n","\n","            # Use the gradient tape to automatically retrieve the gradients of the trainable weights with respect to the loss.\n","            grads = tape.gradient(loss, model.trainable_variables)\n","\n","            # Run one step of gradient descent by updating the value of the weights to minimize the loss.\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","            # Log every 100 batch.\n","            batch_id += 1\n","            if batch_id % 100 == 0:\n","                print('Training time: %0.3f seconds, training loss at sentence %d: %0.4f' % (time.time()-start, batch_id*batch_size, loss/batch_words))\n","                \n","    print ('Epoch %d: Training time=%0.3f seconds, training loss per word=%0.4f' % (epoch, time.time()-start, train_loss/train_words))\n","    \n","    # Evaluate on dev set\n","    dev_words, dev_loss = 0, 0\n","    for sents in dev_gen():\n","        loss = sent_loss(sents)\n","        dev_loss += loss\n","        dev_words += sum(list(map(len, sents)))\n","    print ('Epoch %d: evaluation loss per word=%f' % (epoch, dev_loss/dev_words))\n","    \n","    # Keep track of the development accuracy and reduce the learning rate if it got worse\n","    if last_dev < dev_loss/dev_words:\n","        lr /= 2\n","        optimizer = tf.keras.optimizers.Adam(lr=lr)\n","        print ('learning rate decay to: ', lr)\n","    last_dev = dev_loss/dev_words\n","    \n","    # Save the word vectors\n","    if best_dev > dev_loss/dev_words:\n","        print ('Updating word vectors......')\n","        emb = model.embed.get_weights()[0]\n","        out_v = open(embed_path, 'w')\n","        out_w = open(words_path, 'w')\n","        for i in range(1, nwords+1):\n","            word = reverse_word_index[i]\n","            embedding = emb[i]\n","            out_w.write(word + \"\\n\")\n","            out_v.write('\\t'.join([str(x) for x in embedding]) + \"\\n\")\n","        out_v.close()\n","        out_w.close()\n","        best_dev = dev_loss/dev_words"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start of epoch 0\n","Training time: 23.733 seconds, training loss at sentence 3200: 0.4513\n","Training time: 46.327 seconds, training loss at sentence 6400: 0.4315\n","Training time: 69.267 seconds, training loss at sentence 9600: 0.4180\n","Training time: 92.024 seconds, training loss at sentence 12800: 0.4341\n","Training time: 115.369 seconds, training loss at sentence 16000: 0.4414\n","Training time: 138.974 seconds, training loss at sentence 19200: 0.4368\n","Training time: 163.027 seconds, training loss at sentence 22400: 0.4393\n","Training time: 186.959 seconds, training loss at sentence 25600: 0.4392\n","Training time: 210.437 seconds, training loss at sentence 28800: 0.4298\n","Training time: 234.989 seconds, training loss at sentence 32000: 0.4373\n","Training time: 258.299 seconds, training loss at sentence 35200: 0.4413\n","Training time: 281.996 seconds, training loss at sentence 38400: 0.4104\n","Training time: 307.670 seconds, training loss at sentence 41600: 0.4334\n","Epoch 0: Training time=311.017 seconds, training loss per word=0.4403\n","Epoch 0: evaluation loss per word=0.606399\n","Updating word vectors......\n","Start of epoch 1\n","Training time: 24.200 seconds, training loss at sentence 3200: 0.4181\n","Training time: 50.349 seconds, training loss at sentence 6400: 0.3838\n","Training time: 74.319 seconds, training loss at sentence 9600: 0.3770\n","Training time: 98.269 seconds, training loss at sentence 12800: 0.3764\n","Training time: 125.279 seconds, training loss at sentence 16000: 0.3703\n","Training time: 149.867 seconds, training loss at sentence 19200: 0.3743\n","Training time: 174.548 seconds, training loss at sentence 22400: 0.3874\n","Training time: 199.362 seconds, training loss at sentence 25600: 0.3800\n","Training time: 227.834 seconds, training loss at sentence 28800: 0.3832\n","Training time: 252.940 seconds, training loss at sentence 32000: 0.3861\n","Training time: 278.130 seconds, training loss at sentence 35200: 0.3779\n","Training time: 303.457 seconds, training loss at sentence 38400: 0.3581\n","Training time: 328.777 seconds, training loss at sentence 41600: 0.3657\n","Epoch 1: Training time=332.303 seconds, training loss per word=0.3831\n","Epoch 1: evaluation loss per word=0.714150\n","learning rate decay to:  0.001\n","Start of epoch 2\n","Training time: 25.404 seconds, training loss at sentence 3200: 0.4254\n","Training time: 50.868 seconds, training loss at sentence 6400: 0.4057\n","Training time: 76.466 seconds, training loss at sentence 9600: 0.3984\n","Training time: 102.169 seconds, training loss at sentence 12800: 0.3951\n","Training time: 128.254 seconds, training loss at sentence 16000: 0.3844\n","Training time: 159.730 seconds, training loss at sentence 19200: 0.3954\n","Training time: 186.019 seconds, training loss at sentence 22400: 0.4014\n","Training time: 212.041 seconds, training loss at sentence 25600: 0.4025\n","Training time: 238.797 seconds, training loss at sentence 28800: 0.4006\n","Training time: 265.475 seconds, training loss at sentence 32000: 0.3981\n","Training time: 292.286 seconds, training loss at sentence 35200: 0.3948\n","Training time: 319.168 seconds, training loss at sentence 38400: 0.3745\n","Training time: 346.135 seconds, training loss at sentence 41600: 0.3890\n","Epoch 2: Training time=349.903 seconds, training loss per word=0.3995\n","Epoch 2: evaluation loss per word=0.674997\n","Start of epoch 3\n","Training time: 27.235 seconds, training loss at sentence 3200: 0.4098\n","Training time: 55.362 seconds, training loss at sentence 6400: 0.3875\n","Training time: 83.936 seconds, training loss at sentence 9600: 0.3852\n","Training time: 112.981 seconds, training loss at sentence 12800: 0.3821\n","Training time: 141.427 seconds, training loss at sentence 16000: 0.3719\n","Training time: 170.122 seconds, training loss at sentence 19200: 0.3800\n","Training time: 198.841 seconds, training loss at sentence 22400: 0.3890\n","Training time: 227.188 seconds, training loss at sentence 25600: 0.3914\n","Training time: 263.494 seconds, training loss at sentence 28800: 0.3909\n","Training time: 291.805 seconds, training loss at sentence 32000: 0.3899\n","Training time: 320.273 seconds, training loss at sentence 35200: 0.3850\n","Training time: 348.967 seconds, training loss at sentence 38400: 0.3689\n","Training time: 377.904 seconds, training loss at sentence 41600: 0.3831\n","Epoch 3: Training time=382.014 seconds, training loss per word=0.3880\n","Epoch 3: evaluation loss per word=0.715485\n","learning rate decay to:  0.0005\n","Start of epoch 4\n","Training time: 29.057 seconds, training loss at sentence 3200: 0.4429\n","Training time: 58.530 seconds, training loss at sentence 6400: 0.4257\n","Training time: 88.004 seconds, training loss at sentence 9600: 0.4208\n","Training time: 117.581 seconds, training loss at sentence 12800: 0.4209\n","Training time: 147.487 seconds, training loss at sentence 16000: 0.4140\n","Training time: 186.870 seconds, training loss at sentence 19200: 0.4092\n","Training time: 216.879 seconds, training loss at sentence 22400: 0.4097\n","Training time: 246.858 seconds, training loss at sentence 25600: 0.4111\n","Training time: 277.301 seconds, training loss at sentence 28800: 0.4042\n","Training time: 307.882 seconds, training loss at sentence 32000: 0.4057\n","Training time: 338.163 seconds, training loss at sentence 35200: 0.4048\n","Training time: 369.123 seconds, training loss at sentence 38400: 0.3871\n","Training time: 400.322 seconds, training loss at sentence 41600: 0.4004\n","Epoch 4: Training time=404.685 seconds, training loss per word=0.4146\n","Epoch 4: evaluation loss per word=0.674924\n","Start of epoch 5\n","Training time: 31.349 seconds, training loss at sentence 3200: 0.4232\n","Training time: 62.618 seconds, training loss at sentence 6400: 0.4008\n","Training time: 94.256 seconds, training loss at sentence 9600: 0.4014\n","Training time: 126.472 seconds, training loss at sentence 12800: 0.4002\n","Training time: 159.006 seconds, training loss at sentence 16000: 0.3946\n","Training time: 203.643 seconds, training loss at sentence 19200: 0.3967\n","Training time: 237.098 seconds, training loss at sentence 22400: 0.4007\n","Training time: 270.834 seconds, training loss at sentence 25600: 0.4054\n"],"name":"stdout"}]},{"metadata":{"id":"JIqgcp51EEoW","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}