{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03cbow_ns.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"6069022e-3aaa-49fe-e587-ddeb389b180e","executionInfo":{"status":"ok","timestamp":1554350368821,"user_tz":-480,"elapsed":7212,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":397}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.4)\n","Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.1)\n","/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"9ef25990-8cc8-4108-c83c-c22075855748","executionInfo":{"status":"ok","timestamp":1554350390506,"user_tz":-480,"elapsed":28855,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 16\n","emb_size = 100\n","ngram = 2\n","k = 5 # negative sampling words\n","lr = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/train.txt' # train set\n","dev_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/valid.txt' # dev set\n","embed_path = '/content/drive/My Drive/Colab Notebooks/cs11747/3/embeddings.tsv' # word vector\n","words_path = '/content/drive/My Drive/Colab Notebooks/cs11747/3/labels.tsv' # words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_dataset(filename):\n","    with open(filename, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words = line.lower().strip()\n","            data.append(words)\n","        return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"alGOxQuM5F4R","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = read_dataset(train_path)\n","random.shuffle(train_set)\n","dev_set = read_dataset(dev_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1RkYTXvNRLsz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"07eea087-a3a1-4f82-ede3-7fb36c252795","executionInfo":{"status":"ok","timestamp":1554350390522,"user_tz":-480,"elapsed":28810,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}}},"cell_type":"code","source":["len(train_set)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["42068"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"ryQ1UiUBx8LP","colab_type":"code","colab":{}},"cell_type":"code","source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","outputId":"8d4bfbd7-4b5e-4f96-e5e7-bd96df0d13c8","executionInfo":{"status":"ok","timestamp":1554350391770,"user_tz":-480,"elapsed":30045,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_index = tokenizer.word_index\n","word_counts = tokenizer.word_counts\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","nwords = len(word_index)\n","print (nwords)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["9649\n"],"name":"stdout"}]},{"metadata":{"id":"KwdqPrsJOoWj","colab_type":"code","outputId":"3a68ca81-f807-460e-cb8a-f09bd701c8ac","executionInfo":{"status":"ok","timestamp":1554350391771,"user_tz":-480,"elapsed":30040,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_counts['the']"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50869"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"9gxFzLQ5Ma4O","colab_type":"code","colab":{}},"cell_type":"code","source":["# take the word counts to the 3/4, normalize\n","counts =  map(lambda x: x**.75, word_counts.values())\n","normalizing_constant = sum(counts)\n","word_probabilities = np.zeros(nwords+1)\n","for word in word_counts:\n","    word_probabilities[word_index[word]] = word_counts[word]**.75/normalizing_constant"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m5G-VOxPlmhQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_gen(batch_size=batch_size):\n","    steps = len(train_set) // batch_size\n","    for step in range(steps):\n","        train_seq = tokenizer.texts_to_sequences(train_set[step:step+batch_size])\n","        yield train_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DY4TgO-BDEQ1","colab_type":"code","colab":{}},"cell_type":"code","source":["def dev_gen(batch_size=batch_size):\n","    steps = len(dev_set) // batch_size\n","    for step in range(steps):\n","        dev_seq = tokenizer.texts_to_sequences(dev_set[step:step+batch_size])\n","        yield dev_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CG-wy0-nLTCS","colab_type":"code","colab":{}},"cell_type":"code","source":["class WordEmbCBOW(tf.keras.Model):\n","    def __init__(self, nwords, emb_size):\n","        super(WordEmbCBOW, self).__init__()\n","        self.embed = tf.keras.layers.Embedding(nwords+1, emb_size, trainable=True)\n","\n","    @tf.function\n","    def call(self, x, target, negative_sample=False):\n","        \"\"\"Run the model.\"\"\"\n","        result = self.embed(x)\n","        result = tf.reduce_sum(result, axis=1) #[batch, emb]\n","        if negative_sample:\n","            tar = self.embed(target) #[batch, t, emb]\n","            loss = tf.multiply(tf.expand_dims(result, 1), tar) #[batch, t, emb]\n","            loss = -tf.reduce_sum(loss, axis=-1) #[batch, t]\n","        else:\n","            tar = tf.squeeze(self.embed(tf.expand_dims(target, 1))) #[batch, emb]\n","            loss = tf.multiply(result, tar) #[batch, emb]\n","            loss = tf.reduce_sum(loss, axis=-1) #[batch]\n","\n","        loss = -tf.reduce_sum(tf.math.log_sigmoid(loss)) # neg log likelihood\n","        return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mazHuxpwZQim","colab_type":"code","colab":{}},"cell_type":"code","source":["model = WordEmbCBOW(nwords, emb_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D73f_jv7cis_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Calculate the loss value for the whole batch of sentences\n","def sent_loss(sents):\n","    all_windows = []\n","    all_targets = []\n","    for sent in sents:\n","        padded = [0] * ngram + sent + [0] * ngram\n","        for i in range(ngram, len(sent) + ngram):\n","            window = padded[i-ngram:i] + padded[i+1:i+ngram+1]\n","            all_windows.append(window)\n","            all_targets.append(padded[i])\n","\n","    all_windows = tf.Variable(all_windows, trainable=False)\n","    all_targets = tf.Variable(all_targets, trainable=False)\n","    all_neg_words = tf.Variable(np.random.choice(nwords+1, size=(tf.shape(all_targets)[0], k), replace=True, p=word_probabilities), trainable=False)\n","    pos = model(all_windows, all_targets)\n","    neg = model(all_windows, all_neg_words, True)\n","\n","    return pos + neg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2mH34W35lGVu","colab_type":"code","outputId":"ca9224dd-fa4f-4af9-d893-335792e13c43","executionInfo":{"status":"error","timestamp":1553664330194,"user_tz":-480,"elapsed":1123993,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":809}},"cell_type":"code","source":["last_dev = 1e20\n","best_dev = 1e20\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# Iterate over epochs.\n","for epoch in range(15):\n","    print('Start of epoch %d' % (epoch,))\n","    start = time.time()\n","    train_loss = 0\n","    train_words = 0\n","    batch_id = 0\n","    for sents in train_gen():\n","\n","        # Open a GradientTape to record the operations run during the forward pass, which enables autodifferentiation.\n","        with tf.GradientTape() as tape:\n","\n","            loss = sent_loss(sents)\n","            train_loss += loss\n","            batch_words = sum(list(map(len, sents)))\n","            train_words += batch_words\n","\n","            # Use the gradient tape to automatically retrieve the gradients of the trainable weights with respect to the loss.\n","            grads = tape.gradient(loss, model.trainable_variables)\n","\n","            # Run one step of gradient descent by updating the value of the weights to minimize the loss.\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","            # Log every 50 batch.\n","            batch_id += 1\n","            if batch_id % 50 == 0:\n","                print('Training time: %0.3f seconds, training loss at sentence %d: %0.4f' % (time.time()-start, batch_id*batch_size, loss/batch_words))\n","                \n","    print ('Epoch %d: Training time=%0.3f seconds, training loss per word=%0.4f' % (epoch, time.time()-start, train_loss/train_words))\n","    \n","    # Evaluate on dev set\n","    dev_words, dev_loss = 0, 0\n","    for sents in dev_gen():\n","        loss = sent_loss(sents)\n","        dev_loss += loss\n","        dev_words += sum(list(map(len, sents)))\n","    print ('Epoch %d: evaluation loss per word=%f' % (epoch, dev_loss/dev_words))\n","    \n","    # Keep track of the development accuracy and reduce the learning rate if it got worse\n","    if last_dev < dev_loss/dev_words:\n","        lr /= 2\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","        print ('learning rate decay to: ', lr)\n","    last_dev = dev_loss/dev_words\n","    \n","    # Save the word vectors\n","    if best_dev > dev_loss/dev_words:\n","        print ('Updating word vectors......')\n","        emb = model.embed.get_weights()[0]\n","        out_v = open(embed_path, 'w')\n","        out_w = open(words_path, 'w')\n","        for i in range(1, nwords+1):\n","            word = reverse_word_index[i]\n","            embedding = emb[i]\n","            out_w.write(word + \"\\n\")\n","            out_v.write('\\t'.join([str(x) for x in embedding]) + \"\\n\")\n","        out_v.close()\n","        out_w.close()\n","        best_dev = dev_loss/dev_words"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start of epoch 0\n","Training time: 47.949 seconds, training loss at sentence 800: 4.1238\n","Training time: 94.991 seconds, training loss at sentence 1600: 4.0785\n","Training time: 141.801 seconds, training loss at sentence 2400: 4.0706\n","Training time: 189.704 seconds, training loss at sentence 3200: 3.9821\n","Training time: 236.692 seconds, training loss at sentence 4000: 3.9832\n","Training time: 283.955 seconds, training loss at sentence 4800: 4.0137\n","Training time: 331.900 seconds, training loss at sentence 5600: 3.9267\n","Training time: 381.004 seconds, training loss at sentence 6400: 3.9543\n","Training time: 430.049 seconds, training loss at sentence 7200: 3.9242\n","Training time: 476.824 seconds, training loss at sentence 8000: 3.9358\n","Training time: 526.284 seconds, training loss at sentence 8800: 3.9579\n","Training time: 573.416 seconds, training loss at sentence 9600: 3.9716\n","Training time: 624.046 seconds, training loss at sentence 10400: 3.9129\n","Training time: 671.866 seconds, training loss at sentence 11200: 3.9190\n","Training time: 723.427 seconds, training loss at sentence 12000: 3.9253\n","Training time: 770.944 seconds, training loss at sentence 12800: 3.8610\n","Training time: 823.244 seconds, training loss at sentence 13600: 3.9260\n","Training time: 871.219 seconds, training loss at sentence 14400: 3.9372\n","Training time: 919.485 seconds, training loss at sentence 15200: 3.9316\n","Training time: 972.844 seconds, training loss at sentence 16000: 3.8213\n","Training time: 1021.515 seconds, training loss at sentence 16800: 3.8935\n","Training time: 1069.935 seconds, training loss at sentence 17600: 3.9261\n","Training time: 1124.238 seconds, training loss at sentence 18400: 3.8445\n","Training time: 1172.499 seconds, training loss at sentence 19200: 3.8989\n","Training time: 1221.535 seconds, training loss at sentence 20000: 3.9679\n","Training time: 1270.788 seconds, training loss at sentence 20800: 3.8447\n","Training time: 1327.726 seconds, training loss at sentence 21600: 3.9434\n","Training time: 1376.954 seconds, training loss at sentence 22400: 3.8442\n","Training time: 1426.494 seconds, training loss at sentence 23200: 3.8003\n","Training time: 1476.018 seconds, training loss at sentence 24000: 3.9019\n","Training time: 1525.544 seconds, training loss at sentence 24800: 3.8328\n","Training time: 1584.519 seconds, training loss at sentence 25600: 3.8070\n","Training time: 1634.908 seconds, training loss at sentence 26400: 3.9087\n","Training time: 1685.308 seconds, training loss at sentence 27200: 3.7569\n","Training time: 1735.958 seconds, training loss at sentence 28000: 3.7975\n","Training time: 1786.873 seconds, training loss at sentence 28800: 3.9233\n","Training time: 1837.675 seconds, training loss at sentence 29600: 3.7334\n","Training time: 1899.055 seconds, training loss at sentence 30400: 3.9330\n","Training time: 1950.035 seconds, training loss at sentence 31200: 3.8482\n","Training time: 2000.924 seconds, training loss at sentence 32000: 3.8769\n","Training time: 2052.187 seconds, training loss at sentence 32800: 3.7100\n","Training time: 2103.921 seconds, training loss at sentence 33600: 3.9267\n","Training time: 2155.565 seconds, training loss at sentence 34400: 3.8847\n"],"name":"stdout"}]},{"metadata":{"id":"JIqgcp51EEoW","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}