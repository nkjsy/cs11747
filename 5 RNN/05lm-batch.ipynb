{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05lm-batch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"cc7127de-7e23-4d96-c54c-94cba8710d39","executionInfo":{"status":"ok","timestamp":1554269016030,"user_tz":-480,"elapsed":8743,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":397}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.4)\n","Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.1)\n","/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"c76d5df8-acda-4493-b2ec-ad949fe91051","executionInfo":{"status":"ok","timestamp":1554269037718,"user_tz":-480,"elapsed":30412,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["init_batch_size = 32\n","emb_size = 200\n","hidden_size = 512\n","lr = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"cell_type":"code","source":["train_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/train.txt' # train set\n","dev_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/lm/valid.txt' # dev set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_dataset(filename):\n","    with open(filename, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words = line.lower().strip()\n","            words = '<start> ' + words + ' <end>'\n","            data.append(words)\n","        return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"alGOxQuM5F4R","colab_type":"code","colab":{}},"cell_type":"code","source":["train_set = read_dataset(train_path)\n","dev_set = read_dataset(dev_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ryQ1UiUBx8LP","colab_type":"code","colab":{}},"cell_type":"code","source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\]^_`{|}~ ') \n","tokenizer.fit_on_texts(train_set)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","outputId":"15bb6ebd-aa9d-4527-dcb4-91e6da9aef5f","executionInfo":{"status":"ok","timestamp":1554269039222,"user_tz":-480,"elapsed":31891,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_index = tokenizer.word_index\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","nwords = len(word_index)\n","print (nwords)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["9651\n"],"name":"stdout"}]},{"metadata":{"id":"qP_CCbf8upgZ","colab_type":"code","outputId":"7cdb5119-2547-4137-cbe0-a51178455af7","executionInfo":{"status":"ok","timestamp":1554269039225,"user_tz":-480,"elapsed":31878,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_index['<start>']"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"V3mixgGpC4j0","colab_type":"code","colab":{}},"cell_type":"code","source":["train_seq = tokenizer.texts_to_sequences(train_set)\n","train_seq.sort(key=lambda x: -len(x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xhpQfGKID_1E","colab_type":"code","colab":{}},"cell_type":"code","source":["dev_seq = tokenizer.texts_to_sequences(dev_set)\n","dev_seq.sort(key=lambda x: -len(x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m5G-VOxPlmhQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_gen(init_batch_size=init_batch_size):\n","    i = 0\n","    init_len = len(train_seq[0])\n","    while i<len(train_seq):\n","        batch_size = init_batch_size * init_len // len(train_seq[i])\n","        if i+batch_size < len(train_seq):\n","            batch = tf.keras.preprocessing.sequence.pad_sequences(train_seq[i:i+batch_size], padding='post', maxlen=len(train_seq[i]))\n","        else:\n","            batch = tf.keras.preprocessing.sequence.pad_sequences(train_seq[i:], padding='post', maxlen=len(train_seq[i]))\n","        i += batch_size\n","        yield batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DY4TgO-BDEQ1","colab_type":"code","colab":{}},"cell_type":"code","source":["def dev_gen(init_batch_size=init_batch_size):\n","    i = 0\n","    init_len = len(dev_seq[0])\n","    while i<len(dev_seq):\n","        batch_size = init_batch_size * init_len // len(dev_seq[i])\n","        if i+batch_size < len(dev_seq):\n","            batch = tf.keras.preprocessing.sequence.pad_sequences(dev_seq[i:i+batch_size], padding='post', maxlen=len(dev_seq[i]))\n","        else:\n","            batch = tf.keras.preprocessing.sequence.pad_sequences(dev_seq[i:], padding='post', maxlen=len(dev_seq[i]))\n","        i += batch_size\n","        yield batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CG-wy0-nLTCS","colab_type":"code","colab":{}},"cell_type":"code","source":["class LM(tf.keras.Model):\n","    def __init__(self, embed_size, hidden_size):\n","        super(LM, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(nwords+1, embed_size, trainable=True)\n","        self.gru = tf.keras.layers.GRU(hidden_size, \n","                                       return_sequences=True, \n","                                       return_state=True, \n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc = tf.keras.layers.Dense(nwords+1)\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        # passing one time step to the GRU\n","        output, state = self.gru(x, initial_state=hidden)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        x = self.fc(output)\n","\n","        return x, state"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mazHuxpwZQim","colab_type":"code","colab":{}},"cell_type":"code","source":["model = LM(emb_size, hidden_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d67F0-euFTbP","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D73f_jv7cis_","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_step(sents):\n","    loss = 0  \n","    with tf.GradientTape() as tape:\n","        hidden = None   \n","        # Teacher forcing - feeding the target as the next input\n","        for t in range(sents.shape[1]-1):\n","            step_input = tf.expand_dims(sents[:, t], 1)   \n","            predictions, hidden = model(step_input, hidden)\n","            loss += loss_function(sents[:, t+1], predictions)\n","\n","    batch_loss = loss / int(sents.shape[1]-1)\n","    variables = model.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","  \n","    return batch_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xRDxn80qlBYN","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval_step(sents):\n","    loss = 0  \n","    hidden = None   \n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(sents.shape[1]-1):\n","        step_input = tf.expand_dims(sents[:, t], 1)   \n","        predictions, hidden = model(step_input, hidden)\n","        loss += loss_function(sents[:, t+1], predictions)\n","\n","    batch_loss = loss / int(sents.shape[1]-1)\n","  \n","    return batch_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AKhGD5-EL9yb","colab_type":"code","outputId":"85694df2-c402-4093-981e-e90817c9a0d8","executionInfo":{"status":"ok","timestamp":1554274036757,"user_tz":-480,"elapsed":619,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":1817}},"cell_type":"code","source":["EPOCHS = 20\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    print ('Epoch {} start ...... '.format(epoch + 1))\n","    total_loss = 0\n","    batch = 0\n","    for sents in train_gen():\n","        batch_loss = train_step(sents)\n","        total_loss += batch_loss\n","        batch += 1\n","\n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n","    \n","    # Evaluate on dev set\n","    dev_loss = 0\n","    batch_eval = 0\n","    for sents in dev_gen():\n","        batch_loss = eval_step(sents)\n","        dev_loss += batch_loss\n","        batch_eval += 1\n","\n","    print('Epoch {} finished in {} seconds. Training loss {:.4f}. Evaluation loss {:.4f}.'.format(epoch + 1, time.time() - start, total_loss / batch, dev_loss / batch_eval))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1 start ...... \n","Epoch 1 Batch 100 Loss 6.7293\n","Epoch 1 Batch 200 Loss 6.5328\n","Epoch 1 Batch 300 Loss 5.3926\n","Epoch 1 finished in 207.0101182460785 seconds. Training loss 6.5375. Evaluation loss 7.6251.\n","Epoch 2 start ...... \n","Epoch 2 Batch 100 Loss 6.1411\n","Epoch 2 Batch 200 Loss 5.9658\n","Epoch 2 Batch 300 Loss 5.0788\n","Epoch 2 finished in 195.47878861427307 seconds. Training loss 5.9646. Evaluation loss 6.4226.\n","Epoch 3 start ...... \n","Epoch 3 Batch 100 Loss 5.8104\n","Epoch 3 Batch 200 Loss 5.6663\n","Epoch 3 Batch 300 Loss 4.8821\n","Epoch 3 finished in 196.83963751792908 seconds. Training loss 5.6274. Evaluation loss 6.4168.\n","Epoch 4 start ...... \n","Epoch 4 Batch 100 Loss 5.5786\n","Epoch 4 Batch 200 Loss 5.4648\n","Epoch 4 Batch 300 Loss 4.6743\n","Epoch 4 finished in 205.0242691040039 seconds. Training loss 5.4069. Evaluation loss 6.3162.\n","Epoch 5 start ...... \n","Epoch 5 Batch 100 Loss 5.3691\n","Epoch 5 Batch 200 Loss 5.2574\n","Epoch 5 Batch 300 Loss 4.4994\n","Epoch 5 finished in 199.41108393669128 seconds. Training loss 5.1954. Evaluation loss 5.8449.\n","Epoch 6 start ...... \n","Epoch 6 Batch 100 Loss 5.1695\n","Epoch 6 Batch 200 Loss 5.0933\n","Epoch 6 Batch 300 Loss 4.3791\n","Epoch 6 finished in 193.8454942703247 seconds. Training loss 5.0118. Evaluation loss 5.6611.\n","Epoch 7 start ...... \n","Epoch 7 Batch 100 Loss 5.0037\n","Epoch 7 Batch 200 Loss 4.9524\n","Epoch 7 Batch 300 Loss 4.2716\n","Epoch 7 finished in 200.95416235923767 seconds. Training loss 4.8513. Evaluation loss 5.5197.\n","Epoch 8 start ...... \n","Epoch 8 Batch 100 Loss 4.8522\n","Epoch 8 Batch 200 Loss 4.8065\n","Epoch 8 Batch 300 Loss 4.1890\n","Epoch 8 finished in 203.61152005195618 seconds. Training loss 4.7048. Evaluation loss 5.4192.\n","Epoch 9 start ...... \n","Epoch 9 Batch 100 Loss 4.7101\n","Epoch 9 Batch 200 Loss 4.6713\n","Epoch 9 Batch 300 Loss 4.1143\n","Epoch 9 finished in 196.0003800392151 seconds. Training loss 4.5672. Evaluation loss 5.4238.\n","Epoch 10 start ...... \n","Epoch 10 Batch 100 Loss 4.5761\n","Epoch 10 Batch 200 Loss 4.5538\n","Epoch 10 Batch 300 Loss 4.0595\n","Epoch 10 finished in 202.11154556274414 seconds. Training loss 4.4442. Evaluation loss 5.3499.\n","Epoch 11 start ...... \n","Epoch 11 Batch 100 Loss 4.4499\n","Epoch 11 Batch 200 Loss 4.4512\n","Epoch 11 Batch 300 Loss 4.0091\n","Epoch 11 finished in 198.6404094696045 seconds. Training loss 4.3283. Evaluation loss 5.3743.\n","Epoch 12 start ...... \n","Epoch 12 Batch 100 Loss 4.3210\n","Epoch 12 Batch 200 Loss 4.3413\n","Epoch 12 Batch 300 Loss 3.9661\n","Epoch 12 finished in 196.14352893829346 seconds. Training loss 4.2119. Evaluation loss 5.3744.\n","Epoch 13 start ...... \n","Epoch 13 Batch 100 Loss 4.1975\n","Epoch 13 Batch 200 Loss 4.2394\n","Epoch 13 Batch 300 Loss 3.9251\n","Epoch 13 finished in 198.23487377166748 seconds. Training loss 4.1058. Evaluation loss 5.3705.\n","Epoch 14 start ...... \n","Epoch 14 Batch 100 Loss 4.0767\n","Epoch 14 Batch 200 Loss 4.1428\n","Epoch 14 Batch 300 Loss 3.9042\n","Epoch 14 finished in 209.99481534957886 seconds. Training loss 4.0043. Evaluation loss 5.3634.\n","Epoch 15 start ...... \n","Epoch 15 Batch 100 Loss 3.9507\n","Epoch 15 Batch 200 Loss 4.0407\n","Epoch 15 Batch 300 Loss 3.8941\n","Epoch 15 finished in 193.20676732063293 seconds. Training loss 3.9068. Evaluation loss 5.3841.\n","Epoch 16 start ...... \n","Epoch 16 Batch 100 Loss 3.8489\n","Epoch 16 Batch 200 Loss 3.9503\n","Epoch 16 Batch 300 Loss 3.8545\n","Epoch 16 finished in 195.5961663722992 seconds. Training loss 3.8154. Evaluation loss 5.4044.\n","Epoch 17 start ...... \n","Epoch 17 Batch 100 Loss 3.7475\n","Epoch 17 Batch 200 Loss 3.8708\n","Epoch 17 Batch 300 Loss 3.8232\n","Epoch 17 finished in 206.6130290031433 seconds. Training loss 3.7271. Evaluation loss 5.3671.\n","Epoch 18 start ...... \n","Epoch 18 Batch 100 Loss 3.6406\n","Epoch 18 Batch 200 Loss 3.7896\n","Epoch 18 Batch 300 Loss 3.8171\n","Epoch 18 finished in 196.92445063591003 seconds. Training loss 3.6423. Evaluation loss 5.3765.\n","Epoch 19 start ...... \n","Epoch 19 Batch 100 Loss 3.5438\n","Epoch 19 Batch 200 Loss 3.7069\n","Epoch 19 Batch 300 Loss 3.7852\n","Epoch 19 finished in 194.74037051200867 seconds. Training loss 3.5615. Evaluation loss 5.4515.\n","Epoch 20 start ...... \n","Epoch 20 Batch 100 Loss 3.4563\n","Epoch 20 Batch 200 Loss 3.6357\n","Epoch 20 Batch 300 Loss 3.7576\n","Epoch 20 finished in 206.99018597602844 seconds. Training loss 3.4871. Evaluation loss 5.4343.\n"],"name":"stdout"}]},{"metadata":{"id":"JIqgcp51EEoW","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_step(batch_size):\n","    hidden = tf.Variable(tf.zeros([batch_size, hidden_size]))\n","    #hidden = tf.random.normal(shape=(batch_size, hidden_size), stddev=10)\n","    step_input = tf.random.uniform(shape=(batch_size, 1), maxval=nwords)\n","    #step_input = tf.fill([batch_size, 1], word_index['<start>'])\n","    results = [''] * batch_size\n","    for t in range(100): \n","        predictions, hidden = model(step_input, hidden)\n","        predicted_id = tf.argmax(predictions, axis=1).numpy()\n","\n","        results = [r + reverse_word_index.get(i, '<end>') + ' ' for (r, i) in zip(results, predicted_id)]\n","        \n","        # the predicted ID is fed back into the model\n","        step_input = tf.expand_dims(predicted_id, 1)\n","    return results"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k99PpSQy8wrJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_sents(batch_size):\n","    for i, sent in enumerate(generate_step(batch_size)):\n","        print ('Sentence {} generated: '.format(i))\n","        words = sent.split()\n","        s = ''\n","        for w in words:\n","            if w != '<end>':\n","                s = s + w + ' '\n","            else:\n","                break\n","        print (s)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YQzyNLCx-KbI","colab_type":"code","outputId":"41b0afea-0fc2-4f4f-dfdb-33ec6b1338b0","executionInfo":{"status":"ok","timestamp":1554274093879,"user_tz":-480,"elapsed":2829,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":757}},"cell_type":"code","source":["print_sents(20)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Sentence 0 generated: \n","by comparison the commerce department 's <unk> \n","Sentence 1 generated: \n","henderson deb lynch liberation mather mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather analytical mather \n","Sentence 2 generated: \n","conn spielvogel bates worldwide stake \n","Sentence 3 generated: \n","concede defeat why this is a <unk> \n","Sentence 4 generated: \n","scania ab sydney louisiana pacific telesis worldwide brewing concern \n","Sentence 5 generated: \n","kohl chambers to handle the <unk> \n","Sentence 6 generated: \n","stanley co declined comment on the <unk> \n","Sentence 7 generated: \n","packard battered sweetened wage increases from n \n","Sentence 8 generated: \n","feeding arts michigan michigan michigan district agency \n","Sentence 9 generated: \n","mather setback thatcher administration officials asked him \n","Sentence 10 generated: \n","jones composite index jumped n to n \n","Sentence 11 generated: \n","scenarios that could n't be reached \n","Sentence 12 generated: \n","russia pointed to terminate \n","Sentence 13 generated: \n","why why does you happen \n","Sentence 14 generated: \n","mather deb schroder mixte lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin lorin mixte traub mixte peabody analytical upham analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical mixte analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical upham analytical \n","Sentence 15 generated: \n","how successful women 's \n","Sentence 16 generated: \n","peabody analytical peabody analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical analytical \n","Sentence 17 generated: \n","james foley jr said he was ousted \n","Sentence 18 generated: \n","angeles to restructure \n","Sentence 19 generated: \n","stemming resulting from their <unk> \n"],"name":"stdout"}]},{"metadata":{"id":"luTSAupe-M3g","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}