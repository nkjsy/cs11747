{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11structured_prediction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sIfU8pJW2BDS","colab_type":"code","outputId":"72490799-e6e1-4284-e6a3-285ad3a95472","executionInfo":{"status":"ok","timestamp":1556174178058,"user_tz":-480,"elapsed":6662,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":373}},"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","!pip install tensorflow-gpu==2.0.0-alpha0\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","print (device_name)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n","Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n","Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.5)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.9.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n","/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"DNaSBNNEegl4","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import time\n","import random\n","import math\n","from collections import defaultdict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CMYg_Wuq8M3P","colab_type":"code","outputId":"4466d245-fead-4280-94c4-7b03a9daff8b","executionInfo":{"status":"ok","timestamp":1556174180798,"user_tz":-480,"elapsed":9366,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"EPe-WXaIXwyN","colab_type":"code","colab":{}},"cell_type":"code","source":["# local normalization: teacher forcing, with/without scheduled sampling\n","teacher_forcing = False\n","scheduled_sampling = False\n","# global normalization: structured perceptron or hamming cost augmented hinge loss\n","structured_perceptron = True\n","cost_augmented_hinge = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DG7RwLJjZjdN","colab_type":"code","colab":{}},"cell_type":"code","source":["emb_size = 200\n","tag_emb_size = 64\n","hidden_size = 300\n","lr = 1e-3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bxXRWDeU8MJO","colab_type":"code","colab":{}},"cell_type":"code","source":["# format of files: each line is \"word1|tag1 word2|tag2 ...\"\n","train_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/tag/train.txt' # train set\n","dev_path = '/content/drive/My Drive/Colab Notebooks/cs11747/data/tag/dev.txt' # dev set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_fEUlZyEgEeV","colab_type":"code","colab":{}},"cell_type":"code","source":["w2i = defaultdict(lambda: len(w2i))\n","unk_word = w2i[\"<unk>\"]\n","\n","t2i = defaultdict(lambda: len(t2i))\n","unk_tag = t2i[\"<unk>\"]\n","start_tag = t2i[\"<start>\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x5pWyGKIz5h6","colab_type":"code","colab":{}},"cell_type":"code","source":["def read(fname):\n","    \"\"\"\n","    Read tagged file\n","    \"\"\"\n","    with open(fname, \"r\") as f:\n","        data = []\n","        for line in f:\n","            words, tags = [], []\n","            for wt in line.lower().strip().split():\n","                w, t = wt.split('|')\n","                words.append(w2i[w])\n","                tags.append(t2i[t])\n","            data.append((words, tags))\n","        return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"otHkPYIcjrGj","colab_type":"code","outputId":"dc1b21bd-6a2d-4d3a-bcd5-7403e59955db","executionInfo":{"status":"ok","timestamp":1556174180814,"user_tz":-480,"elapsed":9318,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"cell_type":"code","source":["train = read(train_path)\n","print (f'read in {len(train)} sentences in training set')\n","w2i = defaultdict(lambda: unk_word, w2i)\n","t2i = defaultdict(lambda: unk_tag, t2i)\n","nwords = len(w2i)\n","ntags = len(t2i)\n","print (f'dictionary for words and tags built. {nwords} words and {ntags} tags')\n","dev = read(dev_path)\n","print (f'read in {len(dev)} sentences in dev set')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["read in 10000 sentences in training set\n","dictionary for words and tags built. 23694 words and 11 tags\n","read in 1696 sentences in dev set\n"],"name":"stdout"}]},{"metadata":{"id":"alGOxQuM5F4R","colab_type":"code","outputId":"85f1dfab-0592-4451-cf14-948ae4035fab","executionInfo":{"status":"ok","timestamp":1556174180814,"user_tz":-480,"elapsed":9307,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["w2i['companion']"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"l3hwXL3THHcQ","colab_type":"code","outputId":"df218307-6512-40c0-8bd5-52972ff4c918","executionInfo":{"status":"ok","timestamp":1556174180816,"user_tz":-480,"elapsed":9300,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["w, t = train[10]\n","len(t)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"dbs7HmMcDy1n","colab_type":"code","colab":{}},"cell_type":"code","source":["class Sampler:\n","    '''\n","    A linear decay scheduled sampler. Allow to set always true.\n","    '''\n","    def __init__(self, init_rate=1.0, min_rate=0.2, decay_rate=0.1, all_true=False):\n","        self.min_rate = min_rate\n","        self.decay_rate = decay_rate\n","        self.rate = init_rate\n","        self.reach_min = False\n","        self.all_true = all_true\n","        \n","    def decay(self):\n","        if not self.all_true:\n","            if not self.reach_min:\n","                self.rate -= self.decay_rate\n","                if self.rate < self.min_rate:\n","                    self.rate = self.min_rate\n","                    self.reach_min = True\n","        print (f'sampling rate now is: {self.rate}')\n","        \n","    def sample_true(self):\n","        if self.all_true:\n","            return True\n","        else:\n","            return random.random() < self.rate"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f9l7SACeWLR9","colab_type":"code","colab":{}},"cell_type":"code","source":["if scheduled_sampling:\n","    sampler = Sampler()\n","else:\n","    sampler = Sampler(all_true=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rKYAYxieikEk","colab_type":"code","colab":{}},"cell_type":"code","source":["class Forward(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(Forward, self).__init__()\n","        self.forward = tf.keras.layers.GRU(hidden_size//2, \n","                                       return_sequences=True, \n","                                       return_state=True, \n","                                       recurrent_initializer='glorot_uniform')\n","    \n","    def call(self, x, prev, hidden): # x:[batch=1, 1, emb]   prev:[batch=1, 1, tag_emb]\n","        x = tf.concat([x, prev], axis=-1) #[1, 1, emb+tag_emb]\n","        # passing one time step to the GRU\n","        output, state = self.forward(x, initial_state=hidden)\n","        output = tf.reshape(output, (-1, output.shape[2])) #[batch=1, hidden/2]\n","\n","        return output, state"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7zKf_p-jmVyU","colab_type":"code","colab":{}},"cell_type":"code","source":["class Backward(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(Backward, self).__init__()\n","        self.backward = tf.keras.layers.GRU(hidden_size//2, \n","                                       return_sequences=True, \n","                                       return_state=True, \n","                                       go_backwards=True,\n","                                       recurrent_initializer='glorot_uniform')\n","    \n","    def call(self, x): # x:[batch=1, t, emb]\n","        # passing all time steps to the GRU\n","        outputs, _ = self.backward(x) #[batch=1, t, hidden/2]\n","        outputs = tf.reverse(outputs, axis=[1])\n","        return outputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UHLyn0h1Ce7N","colab_type":"code","colab":{}},"cell_type":"code","source":["class Tagger(tf.keras.Model):\n","    def __init__(self):\n","        super(Tagger, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(nwords, emb_size, trainable=True)\n","        self.tag_embedding = tf.keras.layers.Embedding(ntags, tag_emb_size, trainable=True)\n","            \n","        # forward gru\n","        self.forward = Forward()\n","        \n","        # backward gru\n","        self.back = Backward()\n","            \n","        self.fc = tf.keras.layers.Dense(ntags)\n","        \n","    def call(self, x, ref_tags=None): # x, ref_tags:[batch=1, t] \n","        batch = tf.shape(x)[0] \n","        steps = tf.shape(x)[1]\n","        x = self.embedding(x) #[batch=1, t, emb]\n","        \n","        # backward gru all steps\n","        outputs_b = self.back(x) # outputs of all time steps in backward gru [batch=1, t, hidden/2]\n","        \n","        # forward gru one time step\n","        scores = []\n","        prev = tf.fill([batch, 1], start_tag)\n","        hidden = tf.random.uniform(shape=[batch, hidden_size//2])\n","        for t in range(steps):\n","            inp = tf.expand_dims(x[:, t, :], axis=1)\n","            \n","            prev = self.tag_embedding(prev) #[batch=1, t=1, tag_emb] \n","            \n","            output, hidden = self.forward(inp, prev, hidden)\n","            \n","            comb = tf.concat([output, outputs_b[:, t, :]], axis=-1) # [1, hidden]\n","            score = self.fc(comb) # [1, ntags]\n","            scores.append(score)\n","            prediction = tf.math.argmax(score, axis=-1).numpy()\n","            prediction = tf.expand_dims(prediction, axis=1) # [1,1]\n","\n","            if ref_tags != None:\n","                if sampler.sample_true():\n","                    prev = tf.expand_dims(ref_tags[:, t], axis=1) #[1, 1]\n","                else:\n","                    prev = prediction\n","            else:\n","                prev = prediction\n","        \n","        scores = tf.concat(scores, axis=0) # [t, ntags]\n","        return scores"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ut63VoFrUakb","colab_type":"code","colab":{}},"cell_type":"code","source":["model = Tagger()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vSp0xkiyxBnG","colab_type":"code","colab":{}},"cell_type":"code","source":["def mle(scores, tags):\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    loss = loss_object(tags, scores)\n","    return tf.reduce_mean(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GERl3XemM0o3","colab_type":"code","colab":{}},"cell_type":"code","source":["def hamming_cost(predictions, refs): # get the hamming cost of a given predictions with respect to references\n","    return sum(p != r for p, r in zip(predictions, refs))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BFjgKxWkzEdq","colab_type":"code","colab":{}},"cell_type":"code","source":["def seq_score(scores, tags):\n","    return sum(score[tag] for score, tag in zip(scores, tags))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bk2Cl5N2zl3G","colab_type":"code","colab":{}},"cell_type":"code","source":["def cost_augmented_decode(scores, refs): # to find the predictions that making the cost augmented score higest\n","    augmented = []\n","    for score, ref in zip(scores, refs):\n","        aug = np.ones(score.shape)\n","        aug[ref] = 0\n","        augmented.append(np.argmax(np.add(score, aug)))\n","    return augmented"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RHo7MnRt57bl","colab_type":"code","colab":{}},"cell_type":"code","source":["def structured_prediction_loss(scores, refs):\n","    if structured_perceptron: # structured perceptron\n","        predictions = tf.argmax(scores, axis=1)\n","    if cost_augmented_hinge: # hamming cost augmented hinge loss\n","        predictions = cost_augmented_decode(scores, refs)\n","       \n","    score_ref = seq_score(scores, refs)\n","    score_pred = seq_score(scores, predictions)\n","\n","    if structured_perceptron:\n","        loss = score_pred - score_ref\n","    if cost_augmented_hinge:\n","        hamming = hamming_cost(predictions, refs)\n","        loss = score_pred - score_ref + hamming\n","    return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hydzQV8mQC7S","colab_type":"code","colab":{}},"cell_type":"code","source":["def calc_loss(scores, tags):\n","    if structured_perceptron or cost_augmented_hinge:\n","        return structured_prediction_loss(scores, tags)\n","    if teacher_forcing:\n","        return mle(scores, tags)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8zrWde_2RLN","colab_type":"code","colab":{}},"cell_type":"code","source":["def calc_correct(scores, tags):\n","    correct = [np.argmax(score) == tag for score, tag in zip(scores, tags)]\n","    return sum(correct)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W0QBATOkY6N2","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AKhGD5-EL9yb","colab_type":"code","outputId":"ca12f648-e926-471c-d35e-825b15bb8d11","executionInfo":{"status":"error","timestamp":1556182842647,"user_tz":-480,"elapsed":1049979,"user":{"displayName":"Siyao Jiang","photoUrl":"","userId":"03459425685119894863"}},"colab":{"base_uri":"https://localhost:8080/","height":2712}},"cell_type":"code","source":["EPOCHS = 5\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    print ('=========================================== Epoch {} start ...................................... '.format(epoch + 1))\n","    total_loss = 0\n","    this_correct = this_words = 0\n","    for i in range(len(train)):\n","        words, tags = train[i]\n","        loss = 0\n","        with tf.GradientTape() as tape:\n","            if teacher_forcing:\n","                scores = model(tf.expand_dims(words, axis=0), tf.expand_dims(tags, axis=0))\n","            else:\n","                scores = model(tf.expand_dims(words, axis=0))\n","            loss = calc_loss(scores, tags)\n","            #print (loss)\n","            this_correct += calc_correct(scores, tags)\n","            this_words += len(words)\n","            total_loss += loss\n","            \n","        variables = model.trainable_variables\n","        \n","        gradients = tape.gradient(loss, variables)\n","        optimizer.apply_gradients(zip(gradients, variables))\n","\n","        if (i+1) % 200 == 0:\n","            print('  ---- {} seconds to train {} sentences, loss {:.4f}, accuracy {:.4f}'.format(\n","                time.time()-start, i+1, total_loss/(i+1), this_correct/this_words))\n","    \n","    # Evaluate on dev set\n","    dev_loss = 0\n","    dev_correct = dev_words = 0\n","    for i in range(len(dev)):\n","        words, tags = dev[i]\n","        scores = model(np.expand_dims(words, axis=0))\n","        loss = calc_loss(scores, tags)\n","        dev_correct += calc_correct(scores, tags)\n","        dev_words += len(words)\n","        dev_loss += loss\n","\n","    print('Epoch {} finished in {} seconds'.format(epoch + 1, time.time() - start))\n","    print('Training loss {:.4f}, accuracy {:.4f}'.format(total_loss/len(train), this_correct/this_words))\n","    print('Evaluation loss {:.4f}, accuracy {:.4f}'.format(dev_loss/len(dev), dev_correct/dev_words))\n","    \n","    sampler.decay()"],"execution_count":26,"outputs":[{"output_type":"stream","text":["=========================================== Epoch 1 start ...................................... \n","  ---- 68.08922362327576 seconds to train 200 sentences, loss 0.4457, accuracy 0.7911\n","  ---- 132.5501344203949 seconds to train 400 sentences, loss 0.3210, accuracy 0.8332\n","  ---- 199.58185577392578 seconds to train 600 sentences, loss 0.2730, accuracy 0.8450\n","  ---- 263.1359016895294 seconds to train 800 sentences, loss 0.2392, accuracy 0.8455\n","  ---- 328.5036132335663 seconds to train 1000 sentences, loss 0.2107, accuracy 0.8533\n","  ---- 385.99389362335205 seconds to train 1200 sentences, loss 0.1841, accuracy 0.8590\n","  ---- 448.3567440509796 seconds to train 1400 sentences, loss 0.1677, accuracy 0.8609\n","  ---- 514.2185034751892 seconds to train 1600 sentences, loss 0.1537, accuracy 0.8638\n","  ---- 581.732659816742 seconds to train 1800 sentences, loss 0.1419, accuracy 0.8672\n","  ---- 644.6695346832275 seconds to train 2000 sentences, loss 0.1317, accuracy 0.8665\n","  ---- 704.8125746250153 seconds to train 2200 sentences, loss 0.1230, accuracy 0.8674\n","  ---- 769.8353662490845 seconds to train 2400 sentences, loss 0.1161, accuracy 0.8682\n","  ---- 833.0840675830841 seconds to train 2600 sentences, loss 0.1100, accuracy 0.8687\n","  ---- 894.8892555236816 seconds to train 2800 sentences, loss 0.1035, accuracy 0.8717\n","  ---- 958.1044731140137 seconds to train 3000 sentences, loss 0.0986, accuracy 0.8702\n","  ---- 1017.0887179374695 seconds to train 3200 sentences, loss 0.0940, accuracy 0.8703\n","  ---- 1085.6863331794739 seconds to train 3400 sentences, loss 0.0901, accuracy 0.8703\n","  ---- 1147.9490377902985 seconds to train 3600 sentences, loss 0.0861, accuracy 0.8712\n","  ---- 1214.7695577144623 seconds to train 3800 sentences, loss 0.0825, accuracy 0.8734\n","  ---- 1273.1296381950378 seconds to train 4000 sentences, loss 0.0796, accuracy 0.8735\n","  ---- 1335.7131597995758 seconds to train 4200 sentences, loss 0.0770, accuracy 0.8738\n","  ---- 1397.3710832595825 seconds to train 4400 sentences, loss 0.0743, accuracy 0.8748\n","  ---- 1453.1082608699799 seconds to train 4600 sentences, loss 0.0720, accuracy 0.8749\n","  ---- 1520.9438407421112 seconds to train 4800 sentences, loss 0.0698, accuracy 0.8759\n","  ---- 1586.6096725463867 seconds to train 5000 sentences, loss 0.0680, accuracy 0.8758\n","  ---- 1646.3639216423035 seconds to train 5200 sentences, loss 0.0666, accuracy 0.8743\n","  ---- 1714.8203172683716 seconds to train 5400 sentences, loss 0.0651, accuracy 0.8746\n","  ---- 1793.023924589157 seconds to train 5600 sentences, loss 0.0636, accuracy 0.8767\n","  ---- 1856.3588473796844 seconds to train 5800 sentences, loss 0.0627, accuracy 0.8771\n","  ---- 1923.298623085022 seconds to train 6000 sentences, loss 0.0617, accuracy 0.8766\n","  ---- 1993.5938160419464 seconds to train 6200 sentences, loss 0.0605, accuracy 0.8771\n","  ---- 2059.0533595085144 seconds to train 6400 sentences, loss 0.0595, accuracy 0.8768\n","  ---- 2123.983717918396 seconds to train 6600 sentences, loss 0.0581, accuracy 0.8784\n","  ---- 2189.957765817642 seconds to train 6800 sentences, loss 0.0569, accuracy 0.8782\n","  ---- 2260.916787147522 seconds to train 7000 sentences, loss 0.0559, accuracy 0.8789\n","  ---- 2329.3451206684113 seconds to train 7200 sentences, loss 0.0548, accuracy 0.8797\n","  ---- 2398.7529032230377 seconds to train 7400 sentences, loss 0.0540, accuracy 0.8800\n","  ---- 2466.4703147411346 seconds to train 7600 sentences, loss 0.0531, accuracy 0.8802\n","  ---- 2527.7858152389526 seconds to train 7800 sentences, loss 0.0524, accuracy 0.8801\n","  ---- 2588.810309648514 seconds to train 8000 sentences, loss 0.0523, accuracy 0.8797\n","  ---- 2650.788377046585 seconds to train 8200 sentences, loss 0.0515, accuracy 0.8804\n","  ---- 2715.9612452983856 seconds to train 8400 sentences, loss 0.0508, accuracy 0.8806\n","  ---- 2782.915321826935 seconds to train 8600 sentences, loss 0.0503, accuracy 0.8795\n","  ---- 2845.0560660362244 seconds to train 8800 sentences, loss 0.0496, accuracy 0.8803\n","  ---- 2912.6493394374847 seconds to train 9000 sentences, loss 0.0489, accuracy 0.8806\n","  ---- 2980.0411026477814 seconds to train 9200 sentences, loss 0.0484, accuracy 0.8805\n","  ---- 3049.3535261154175 seconds to train 9400 sentences, loss 0.0479, accuracy 0.8807\n","  ---- 3105.5638451576233 seconds to train 9600 sentences, loss 0.0472, accuracy 0.8811\n","  ---- 3173.3770084381104 seconds to train 9800 sentences, loss 0.0465, accuracy 0.8815\n","  ---- 3245.850337266922 seconds to train 10000 sentences, loss 0.0461, accuracy 0.8816\n","Epoch 1 finished in 3452.2051582336426 seconds\n","Training loss 0.0461, accuracy 0.8816\n","Evaluation loss 0.0372, accuracy 0.8482\n","sampling rate now is: 1.0\n","=========================================== Epoch 2 start ...................................... \n","  ---- 67.01395773887634 seconds to train 200 sentences, loss 0.0200, accuracy 0.8868\n","  ---- 131.41111755371094 seconds to train 400 sentences, loss 0.0165, accuracy 0.9124\n","  ---- 198.2625949382782 seconds to train 600 sentences, loss 0.0163, accuracy 0.9169\n","  ---- 260.97890400886536 seconds to train 800 sentences, loss 0.0158, accuracy 0.9193\n","  ---- 327.2977523803711 seconds to train 1000 sentences, loss 0.0155, accuracy 0.9255\n","  ---- 386.9972710609436 seconds to train 1200 sentences, loss 0.0144, accuracy 0.9286\n","  ---- 447.67351603507996 seconds to train 1400 sentences, loss 0.0149, accuracy 0.9283\n","  ---- 513.7772064208984 seconds to train 1600 sentences, loss 0.0147, accuracy 0.9298\n","  ---- 580.2372379302979 seconds to train 1800 sentences, loss 0.0149, accuracy 0.9305\n","  ---- 644.2915861606598 seconds to train 2000 sentences, loss 0.0155, accuracy 0.9294\n","  ---- 707.1507403850555 seconds to train 2200 sentences, loss 0.0154, accuracy 0.9301\n","  ---- 770.0280277729034 seconds to train 2400 sentences, loss 0.0160, accuracy 0.9299\n","  ---- 833.3109602928162 seconds to train 2600 sentences, loss 0.0168, accuracy 0.9293\n","  ---- 894.7735500335693 seconds to train 2800 sentences, loss 0.0168, accuracy 0.9306\n","  ---- 958.4818620681763 seconds to train 3000 sentences, loss 0.0172, accuracy 0.9297\n","  ---- 1019.8089818954468 seconds to train 3200 sentences, loss 0.0173, accuracy 0.9295\n","  ---- 1086.3994662761688 seconds to train 3400 sentences, loss 0.0177, accuracy 0.9293\n","  ---- 1148.2906377315521 seconds to train 3600 sentences, loss 0.0177, accuracy 0.9297\n","  ---- 1216.165833234787 seconds to train 3800 sentences, loss 0.0175, accuracy 0.9311\n","  ---- 1275.4006259441376 seconds to train 4000 sentences, loss 0.0177, accuracy 0.9307\n","  ---- 1338.745439529419 seconds to train 4200 sentences, loss 0.0178, accuracy 0.9309\n","  ---- 1398.1931529045105 seconds to train 4400 sentences, loss 0.0177, accuracy 0.9310\n","  ---- 1454.119714975357 seconds to train 4600 sentences, loss 0.0178, accuracy 0.9309\n","  ---- 1521.9643511772156 seconds to train 4800 sentences, loss 0.0180, accuracy 0.9313\n","  ---- 1588.537032842636 seconds to train 5000 sentences, loss 0.0182, accuracy 0.9309\n","  ---- 1650.1822757720947 seconds to train 5200 sentences, loss 0.0188, accuracy 0.9295\n","  ---- 1716.3876421451569 seconds to train 5400 sentences, loss 0.0190, accuracy 0.9297\n","  ---- 1794.4965875148773 seconds to train 5600 sentences, loss 0.0190, accuracy 0.9306\n","  ---- 1858.3321125507355 seconds to train 5800 sentences, loss 0.0190, accuracy 0.9308\n","  ---- 1925.4977822303772 seconds to train 6000 sentences, loss 0.0194, accuracy 0.9301\n","  ---- 1995.8674206733704 seconds to train 6200 sentences, loss 0.0195, accuracy 0.9304\n","  ---- 2061.3979337215424 seconds to train 6400 sentences, loss 0.0198, accuracy 0.9299\n","  ---- 2126.2299330234528 seconds to train 6600 sentences, loss 0.0197, accuracy 0.9307\n","  ---- 2191.8892459869385 seconds to train 6800 sentences, loss 0.0198, accuracy 0.9304\n","  ---- 2265.4993970394135 seconds to train 7000 sentences, loss 0.0199, accuracy 0.9306\n","  ---- 2331.9615609645844 seconds to train 7200 sentences, loss 0.0197, accuracy 0.9311\n","  ---- 2401.363043785095 seconds to train 7400 sentences, loss 0.0198, accuracy 0.9311\n","  ---- 2468.8017139434814 seconds to train 7600 sentences, loss 0.0200, accuracy 0.9309\n","  ---- 2530.9796504974365 seconds to train 7800 sentences, loss 0.0201, accuracy 0.9306\n","  ---- 2592.2754662036896 seconds to train 8000 sentences, loss 0.0210, accuracy 0.9300\n","  ---- 2654.2351744174957 seconds to train 8200 sentences, loss 0.0211, accuracy 0.9304\n","  ---- 2719.2093937397003 seconds to train 8400 sentences, loss 0.0212, accuracy 0.9304\n","  ---- 2785.9464750289917 seconds to train 8600 sentences, loss 0.0216, accuracy 0.9296\n","  ---- 2848.689031124115 seconds to train 8800 sentences, loss 0.0215, accuracy 0.9300\n","  ---- 2914.7904233932495 seconds to train 9000 sentences, loss 0.0215, accuracy 0.9301\n","  ---- 2983.11087846756 seconds to train 9200 sentences, loss 0.0216, accuracy 0.9298\n","  ---- 3052.2657754421234 seconds to train 9400 sentences, loss 0.0218, accuracy 0.9298\n","  ---- 3109.233038187027 seconds to train 9600 sentences, loss 0.0218, accuracy 0.9300\n","  ---- 3178.1017229557037 seconds to train 9800 sentences, loss 0.0217, accuracy 0.9303\n","  ---- 3247.7378368377686 seconds to train 10000 sentences, loss 0.0217, accuracy 0.9303\n","Epoch 2 finished in 3452.050511598587 seconds\n","Training loss 0.0217, accuracy 0.9303\n","Evaluation loss 0.0823, accuracy 0.8332\n","sampling rate now is: 1.0\n","=========================================== Epoch 3 start ...................................... \n","  ---- 66.82764029502869 seconds to train 200 sentences, loss 0.0372, accuracy 0.9122\n","  ---- 129.86571431159973 seconds to train 400 sentences, loss 0.0246, accuracy 0.9347\n","  ---- 195.55262517929077 seconds to train 600 sentences, loss 0.0223, accuracy 0.9359\n","  ---- 259.344744682312 seconds to train 800 sentences, loss 0.0222, accuracy 0.9341\n","  ---- 326.00890374183655 seconds to train 1000 sentences, loss 0.0218, accuracy 0.9372\n","  ---- 385.087819814682 seconds to train 1200 sentences, loss 0.0203, accuracy 0.9394\n","  ---- 445.7322778701782 seconds to train 1400 sentences, loss 0.0206, accuracy 0.9392\n","  ---- 510.35827255249023 seconds to train 1600 sentences, loss 0.0204, accuracy 0.9396\n","  ---- 578.7802355289459 seconds to train 1800 sentences, loss 0.0206, accuracy 0.9399\n","  ---- 642.9221105575562 seconds to train 2000 sentences, loss 0.0207, accuracy 0.9394\n","  ---- 704.6019768714905 seconds to train 2200 sentences, loss 0.0206, accuracy 0.9392\n","  ---- 767.3332452774048 seconds to train 2400 sentences, loss 0.0210, accuracy 0.9392\n","  ---- 829.4157457351685 seconds to train 2600 sentences, loss 0.0222, accuracy 0.9384\n","  ---- 892.661046743393 seconds to train 2800 sentences, loss 0.0216, accuracy 0.9403\n","  ---- 956.2866849899292 seconds to train 3000 sentences, loss 0.0221, accuracy 0.9392\n","  ---- 1016.9214141368866 seconds to train 3200 sentences, loss 0.0222, accuracy 0.9389\n","  ---- 1082.859001159668 seconds to train 3400 sentences, loss 0.0223, accuracy 0.9392\n","  ---- 1144.9144489765167 seconds to train 3600 sentences, loss 0.0221, accuracy 0.9397\n","  ---- 1213.6617352962494 seconds to train 3800 sentences, loss 0.0217, accuracy 0.9410\n","  ---- 1272.6964328289032 seconds to train 4000 sentences, loss 0.0218, accuracy 0.9409\n","  ---- 1336.9312198162079 seconds to train 4200 sentences, loss 0.0216, accuracy 0.9415\n","  ---- 1395.202225446701 seconds to train 4400 sentences, loss 0.0211, accuracy 0.9420\n","  ---- 1452.220495223999 seconds to train 4600 sentences, loss 0.0212, accuracy 0.9418\n","  ---- 1521.1395256519318 seconds to train 4800 sentences, loss 0.0211, accuracy 0.9425\n","  ---- 1588.2258565425873 seconds to train 5000 sentences, loss 0.0214, accuracy 0.9422\n","  ---- 1648.13280916214 seconds to train 5200 sentences, loss 0.0222, accuracy 0.9408\n","  ---- 1714.0934195518494 seconds to train 5400 sentences, loss 0.0226, accuracy 0.9406\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-72620d28d2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_StridedSliceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m       shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice_grad\u001b[0;34m(shape, begin, end, strides, dy, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   9995\u001b[0m         \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9996\u001b[0m         \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9997\u001b[0;31m         new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[1;32m   9998\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9999\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"F87ScMpy98PR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}